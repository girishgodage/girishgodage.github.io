<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.6">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-10-07T11:34:04+00:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Girish Godage</title><subtitle>I am a passionate Leader, who has a good command in technology &amp; Mangement. Also, I am a creative designer and an innovatie techie.</subtitle><author><name>Girish Godage</name></author><entry><title type="html">Create user flows in Azure Active Directory B2C</title><link href="http://localhost:4000/blog/azure-acticedirectoryB2C_CreateUserFlow" rel="alternate" type="text/html" title="Create user flows in Azure Active Directory B2C" /><published>2020-03-12T12:41:00+00:00</published><updated>2020-03-12T12:41:00+00:00</updated><id>http://localhost:4000/blog/azure-activedirectoryB2C_CreateUserFlow</id><content type="html" xml:base="http://localhost:4000/blog/azure-acticedirectoryB2C_CreateUserFlow">&gt;## Create user flows in Azure Active Directory B2C

In your applications you may have user flows that enable users to sign up, sign in, or manage their profile. You can create multiple user flows of different types in your Azure Active Directory B2C (Azure AD B2C) tenant and use them in your applications as needed. User flows can be reused across applications.

In this article, you learn how to:

* Create a sign-up and sign-in user flow
* Create a profile editing user flow
* Create a password reset user flow
  
This tutorial shows you how to create some recommended user flows by using the Azure portal.

If you don't have an Azure subscription, create a [free account](https://azure.microsoft.com/free/?WT.mc_id=A261C142F) before you begin.

## Prerequisites

If you haven't already created your own [Azure AD B2C Tenant](azure-activedirectoryB2C_CreateTenant), create one now. You can use an existing Azure AD B2C tenant.

## Create a sign-up and sign-in user flow

The sign-up and sign-in user flow handles both sign-up and sign-in experiences with a single configuration. Users of your application are led down the right path depending on the context.

1. Sign in to the [Azure portal](https://portal.azure.com/).

2. Select the **Directory + Subscription** icon in the portal toolbar, and then select the directory that contains your Azure AD B2C tenant.

![image info](/img/azure/6/directory-subscription-pane.png)

3. In the Azure portal, search for and select **Azure AD B2C**.

4. Under **Policies**, select **User flows**, and then select **New user flow**.

![image info](/img/azure/6/signup-signin-user-flow.png)

5. On the Create a user flow page, select the Sign up and sign in user flow.

![image info](/img/azure/6/select-user-flow-type.png)

6.Under Select a version, select Recommended, and then select Create.

![image info](/img/azure/6/select-version.png)

7. Enter a **Name** for the user flow. For example, signupsignin1.

8. For **Identity providers**, select **Email signup**.

9. For User **attributes and claims**, choose the claims and attributes that you want to collect and send from the user during sign-up. For example, select **Show more**, and then choose attributes and claims for **Country/Region, Display Name, and Postal Code**. Click **OK**.

![image info](/img/azure/6/signup-signin-attributes.png)

10. Click **Create** to add the user flow. A prefix of B2C_1 is automatically prepended to the name.


## Test the user flow

1. Select the user flow you created to open its overview page, then select **Run user flow**.

2. For **Application**, select the web application named webapp1 that you previously registered. The **Reply URL** should show https://jwt.ms.

3. Click **Run user flow**, and then **select Sign up now**.

![image info](/img/azure/6/signup-signin-run-now.png)

4. Enter a valid email address, click Send **verification code**, enter the verification code that you receive, then select **Verify code**.

5. Enter a new password and confirm the password.

6. Select your country and region, enter the name that you want displayed, enter a postal code, and then click **Create**. The token is returned to https://jwt.ms and should be displayed to you.

7. You can now run the user flow again and you should be able to sign in with the account that you created. The returned token includes the claims that you selected of country/region, name, and postal code

## Create a profile editing user flow

If you want to enable users to edit their profile in your application, you use a profile editing user flow.

1. In the menu of the Azure AD B2C tenant overview page, select **User flows**, and then select **New user flow**.
2. On the **Create a user flow** page, select the **Profile editing** user flow.
3. Under **Select a version**, select **Recommended**, and then select **Create**.
4. Enter a **Name** for the user flow. For example, profileediting1.
5. For **Identity providers**, select **Local Account SignIn**.
6. For **User attributes**, choose the attributes that you want the customer to be able to edit in their profile. For example, select **Show more**, and then choose both **attributes and claims for Display name and Job title**. Click **OK**.
7. Click **Create** to add the user flow. A prefix of B2C_1 is automatically appended to the name.

## Test the user flow

1. Select the user flow you created to open its overview page, then select **Run user flow**.
2. For **Application**, select the web application named webapp1 that you previously registered. The **Reply URL** should show https://jwt.ms.
3. Click **Run user flow**, and then sign in with the account that you previously created.
4. You now have the opportunity to change the display name and job title for the user. Click **Continue**. The token is returned to https://jwt.ms and should be displayed to you.

## Create a password reset user flow

To enable users of your application to reset their password, you use a password reset user flow.

1. In the Azure AD B2C tenant overview menu, select **User flows**, and then select **New user flow**.
2. On the **Create a user flow** page, select the **Password reset** user flow.
3. Under **Select** a version, select **Recommended**, and then select **Create**.
4. Enter a **Name** for the user flow. For example, passwordreset1.
5. For **Identity providers**, enable **Reset password using email address**.
6. Under **Application claims**, click **Show more** and choose the claims that you want returned in the authorization tokens sent back to your application. For example, select User's Object ID.
7. Click **OK**.
8. Click **Create** to add the user flow. A prefix of B2C_1 is automatically appended to the name.
   
## Test the user flow

1. Select the user flow you created to open its overview page, then select **Run user flow**.
2. For **Application**, select the web application named webapp1 that you previously registered. The **Reply URL** should show https://jwt.ms.
3. Click **Run user flow**, verify the email address of the account that you previously created, and select **Continue**.
4. You now have the opportunity to change the password for the user. Change the password and select **Continue**. The token is returned to https://jwt.ms and should be displayed to you.

## Next steps

In this article, you learned how to:

* Create a sign-up and sign-in user flow
* Create a profile editing user flow
* Create a password reset user flow
  
Next, learn about adding identity providers to your applications to enable user sign-in with providers like Azure AD, Amazon, Facebook, GitHub, LinkedIn, Microsoft, or Twitter.</content><author><name>Girish Godage</name></author><category term="learning" /><summary type="html">Create user flows in Azure Active Directory B2C</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/img/azureAD.png" /></entry><entry><title type="html">Register a web application in Azure Active Directory B2C</title><link href="http://localhost:4000/blog/azure-acticedirectoryB2C_RegisterApp" rel="alternate" type="text/html" title="Register a web application in Azure Active Directory B2C" /><published>2020-03-12T12:41:00+00:00</published><updated>2020-03-12T12:41:00+00:00</updated><id>http://localhost:4000/blog/azure-activedirectoryB2C_RegisterApp</id><content type="html" xml:base="http://localhost:4000/blog/azure-acticedirectoryB2C_RegisterApp">&gt;## Register a web application in Azure Active Directory B2C

Before your applications can interact with Azure Active Directory B2C (Azure AD B2C), they must be registered in a tenant that you manage. This tutorial shows you how to register a web application using the Azure portal.

In this article, you learn how to:

* Register a web application
* Create a client secret
  
If you're using a native app instead (e.g. iOS, Android, mobile &amp; desktop), learn how to register a native client application.

If you don't have an Azure subscription, create a [free account](https://azure.microsoft.com/free/?WT.mc_id=A261C142F) before you begin.

## Prerequisites

If you haven't already created your own [Azure AD B2C Tenant](azure-activedirectoryB2C_CreateTenant), create one now. You can use an existing Azure AD B2C tenant.

## Register a web application

To register an application in your Azure AD B2C tenant, you can use new unified App registrations experience 

1. Sign in to the [Azure portal](https://portal.azure.com/).

2. Select the **Directory + Subscription** icon in the portal toolbar, and then select the directory that contains your Azure AD B2C tenant.

3. In the Azure portal, search for and select **Azure AD B2C**.

4. Select **App registrations**, and then select **New registration**.

5. Enter a **Name** for the application. For example, webapp1.

6. Under **Supported account types**, select **Accounts in any organizational directory or any identity provider. For authenticating users with Azure AD B2C.**

7. Under **Redirect URI**, select **Web**, and then enter https://jwt.ms in the URL text box.

The redirect URI is the endpoint to which the user is sent by the authorization server (Azure AD B2C, in this case) after completing its interaction with the user, and to which an access token or authorization code is sent upon successful authorization. In a production application, it's typically a publicly accessible endpoint where your app is running, like https://contoso.com/auth-response. For testing purposes like this tutorial, you can set it to https://jwt.ms, a Microsoft-owned web application that displays the decoded contents of a token (the contents of the token never leave your browser). During app development, **you might add the endpoint where your application listens locally, like https://localhost:5000**. You can add and modify redirect URIs in your registered applications at any time.

The following restrictions apply to redirect URIs:

   * The reply URL must begin with the scheme https.
   * The reply URL is case-sensitive. Its case must match the case of the URL path of your running application. For example, if your application includes as part of its path .../abc/response-oidc, do not specify .../ABC/response-oidc in the reply URL. Because the web browser treats paths as case-sensitive, cookies associated with .../abc/response-oidc may be excluded if redirected to the case-mismatched .../ABC/response-oidc URL.
  
  
8. Under Permissions, select the Grant admin consent to openid and offline_access permissions check box.

9. Select Register.

Once the application registration is complete, enable the implicit grant flow:

## Create a client secret

If your application exchanges an authorization code for an access token, you need to create an application secret.

1. In the **Azure AD B2C - App registrations** page, select the application you created, for example webapp1.
2. In the left menu, under **Manage**, select **Certificates &amp; secrets**.
3. Select **New client secret**.
4. Enter a description for the client secret in the **Description** box. For example, clientsecret1.
5. Under **Expires**, select a duration for which the secret is valid, and then select **Add**.
6. Record the secret's **Value**. You use this value as the application secret in your application's code.

## Next steps
In this article, you learned how to:

* Register a web application
* Create a client secret
  
Next, learn how to [create user flows](azure-acticedirectoryB2C_CreateUserFlow) to enable your users to sign up, sign in, and manage their profiles.</content><author><name>Girish Godage</name></author><category term="learning" /><summary type="html">Register a web application in Azure Active Directory B2C</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/img/azureAD.png" /></entry><entry><title type="html">Create an Azure AD B2C tenant</title><link href="http://localhost:4000/blog/azure-acticedirectoryB2C_CreateTenant" rel="alternate" type="text/html" title="Create an Azure AD B2C tenant" /><published>2020-03-12T10:41:00+00:00</published><updated>2020-03-12T10:41:00+00:00</updated><id>http://localhost:4000/blog/azure-activedirectoryB2C_CreateTenant</id><content type="html" xml:base="http://localhost:4000/blog/azure-acticedirectoryB2C_CreateTenant">&gt;## Create an Azure AD B2C tenant

Before your applications can interact with Azure Active Directory B2C (Azure AD B2C), **they must be registered in a tenant that you manage**.

In this article, you learn how to:

* Create an Azure AD B2C tenant
* Link your tenant to your subscription
* Switch to the directory containing your Azure AD B2C tenant
* Add the Azure AD B2C resource as a Favorite in the Azure portal

## Prerequisites

If you don't have an Azure subscription, create a [free account](https://azure.microsoft.com/free/?WT.mc_id=A261C142F) before you begin.

## Create an Azure AD B2C tenant

1. Sign in to the [Azure portal](https://portal.azure.com/). Sign in with an Azure account that's been assigned at least the [Contributor](https://docs.microsoft.com/en-in/azure/role-based-access-control/built-in-roles) role within the subscription or a resource group within the subscription.

2. Select the directory that contains your subscription.

In the Azure portal toolbar, select the Directory + Subscription icon, and then select the directory that contains your subscription. This directory is different from the one that will contain your Azure AD B2C tenant.

![image info](/img/azure/4/portal-01-pick-directory.png)

3. On the Azure portal menu or from the **Home** page, select **Create a resource**.

4. Search for **Azure Active Directory B2C**, and then select **Create**.

5. Select **Create a new Azure AD B2C Tenant**.

![image info](/img/azure/4/portal-02-create-tenant.png)

6. On the **Create a directory** page, enter the following:

* **Organization name** - Enter a name for your Azure AD B2C tenant.
* **Initial domain name** - Enter a domain name for your Azure AD B2C tenant.
* **Country or region** - Select your country or region from the list. This selection can't be changed later.
* **Subscription** - Select your subscription from the list.
  * **Resource group** - Select a resource group that will contain the tenant. Or select **Create new**, enter a **Name** for the resource group, select the **Resource group location**, and then select **OK**. 

![image info](/img/azure/4/review-and-create-tenant.png)

7. Select **Review + create**.

8. Review your directory settings. Then select **Create**.

You can link multiple Azure AD B2C tenants to a single Azure subscription for billing purposes. To link a tenant, you must be an admin in the Azure AD B2C tenant and be assigned at least a Contributor role within the Azure subscription.

## Select your B2C tenant directory

To start using your new Azure AD B2C tenant, you need to switch to the directory that contains the tenant.

Select the **Directory + subscription** filter in the top menu of the Azure portal, then select the directory that contains your Azure AD B2C tenant.

If at first you don't see your new Azure B2C tenant in the list, refresh your browser window, then select the **Directory + subscription** filter again in the top menu.

![image info](/img/azure/4/portal-07-select-tenant-directory.png)

## Add Azure AD B2C as a favorite (optional)

This optional step makes it easier to select your Azure AD B2C tenant in the following and all subsequent tutorials.

Instead of searching for Azure AD B2C in **All services** every time you want to work with your tenant, you can instead favorite the resource. Then, you can select it from the portal menu's **Favorites** section to quickly browse to your Azure AD B2C tenant.

You only need to perform this operation once. Before performing these steps, make sure you've switched to the directory containing your Azure AD B2C tenant as described in the previous section, [Select your B2C tenant directory](azure-acticedirectoryB2C_CreateTenant#select-your-b2c-tenant-directory).

1. Sign in to the [Azure portal](https://portal.azure.com/).

2. In the Azure portal menu, select **All services**.

3. In the **All services** search box, search for Azure AD B2C, hover over the search result, and then select the star icon in the tooltip. Azure AD B2C now appears in the Azure portal under Favorites.

4. If you want to change the position of your new favorite, go to the Azure portal menu, select Azure AD B2C, and then drag it up or down to the desired position.

![image info](/img/azure/4/portal-08-b2c-favorite.png)

## Next steps

In this article, you learned how to:

* Create an Azure AD B2C tenant
* Link your tenant to your subscription
* Switch to the directory containing your Azure AD B2C tenant
* Add the Azure AD B2C resource as a Favorite in the Azure portal
* 
Next, learn [how to register a web application](azure-acticedirectoryB2C_RegisterApp) in your new tenant.</content><author><name>Girish Godage</name></author><category term="learning" /><summary type="html">Create an Azure AD B2C tenant</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/img/azureAD.png" /></entry><entry><title type="html">Azure Active Directory B2C</title><link href="http://localhost:4000/blog/azure-acticedirectoryB2C" rel="alternate" type="text/html" title="Azure Active Directory B2C" /><published>2020-03-10T10:41:00+00:00</published><updated>2020-03-10T10:41:00+00:00</updated><id>http://localhost:4000/blog/azure-activedirectoryB2C</id><content type="html" xml:base="http://localhost:4000/blog/azure-acticedirectoryB2C">&gt;## What is Azure Active Directory B2C?

 **Azure Active Directory B2C** provides business-to-customer **identity as a service**. Your customers use their preferred social, enterprise, or local account identities to get single sign-on access to your applications and APIs.

![image info](/img/azure/3/azureadb2c-overview.png)
 

**Azure Active Directory B2C (Azure AD B2C)** is a customer identity access management (CIAM) solution capable of **supporting millions of users and billions of authentications per day**. 
It takes care of the **scaling** and **safety** of the *authentication platform*, monitoring and automatically handling threats like **denial-of-service, password spray, or brute force attacks**.

## Custom-branded identity solution

 Azure AD B2C is a **white-label authentication solution**. You can customize the entire user experience with your brand so that it blends seamlessly with your web and mobile applications.

**Customize every page displayed by Azure AD B2C when your users sign up, sign in, and modify their profile information**. Customize the **HTML, CSS, and JavaScript** in your user journeys so that the Azure AD B2C experience **looks and feels like it's a native part of your application**. 

![image info](/img/azure/3/sign-in-small.png)

## Single sign-on access with a user-provided identity

Azure AD B2C uses standards-based authentication protocols including **OpenID Connect, OAuth 2.0, and SAML**. It integrates with most modern applications and commercial off-the-shelf software.

![image info](/img/azure/3/scenario-singlesignon.png)

By serving as the **central authentication authority** for your ***web applications, mobile apps, and API***s, Azure AD B2C enables you to build a single **sign-on (SSO) solution** for them all. Centralize the collection of **user profile and preference information**, and *capture detailed analytics* about **sign-in behavior** and **sign-up conversion**.

## Integrate with external user stores

Azure AD B2C provides a directory that **can hold 100 custom attributes per user**. However, you can also **integrate with external systems**. For example, use Azure AD B2C for **authentication**, but delegate to an external **customer relationship management (CRM) ** or **customer loyalty database** as the source of truth for customer data.

Another external user store scenario is to have Azure AD B2C handle the authentication for your application, but **integrate with an external system that stores user profile or personal data**. For example, to satisfy data residency requirements like **regional or on-premises data** storage policies.

![image info](/img/azure/3/scenario-remoteprofile.png)

Azure AD B2C can facilitate **collecting the information from the user during registration or profile editing**, then *hand that data off to the external system*. Then, during future **authentications**, Azure AD B2C can retrieve the data from the external system and, if needed, include it as a part of the authentication token response it sends to your application.

## Progressive profiling

Another user journey option includes **progressive profiling**. Progressive profiling allows your **customers to quickly complete their first transaction by collecting a minimal amount of information**. Then, gradually collect more profile data from the customer on future sign-ins.

![image info](/img/azure/3/scenario-progressive.png)

## Third-party identity verification and proofing

Use Azure AD B2C to facilitate *identity verification and proofing by collecting user data, then passing it to a third party system to perform validation, trust scoring, and approval for user account creation*.

![image info](/img/azure/3/scenario-idproofing.png)

These are just some of the things you can do with Azure AD B2C as your business-to-customer identity platform.

&gt; ## Technical and feature overview of Azure Active Directory B2C

Here are the **primary resources** you work with in the service, its features, and how these enable you to provide a fully custom identity experience for your customers in your applications.

## Azure AD B2C tenant

In Azure Active Directory B2C (Azure AD B2C), **a tenant** represents **your organization** and is a **directory of users**. *Each Azure AD B2C tenant is distinct and separate from other Azure AD B2C tenants*. An Azure AD B2C tenant is different than an Azure Active Directory tenant, which you may already have.

The primary resources you work with in an Azure AD B2C tenant are:

* **Directory** - The directory is where Azure AD B2C **stores** your **users' credentials** and **profile data**, as well as your **application registrations**.
  
* **Application registrations** - You **register your web, mobile, and native applications** with Azure AD B2C to **enable identity management**. Also, **any APIs** you want to protect with Azure AD B2C.
* **User flows and custom policies** - The built-in **(user flows)** and fully customizable **(custom policies)** identity experiences for your applications.
    * Use user flows for **quick configuration and enablement of common identity tasks like sign up, sign in, and profile editing**.
    * Use **custom policies** to enable user experiences not only for the **common identity tasks**, but also for crafting support for **complex identity workflows** unique to your organization, customers, employees, partners, and citizens.
* **Identity providers** - Federation settings for:
    * **Social identity** providers like Facebook, LinkedIn, or Twitter that you want to support in your applications.
    * **External identity** providers that support standard identity protocols like OAuth 2.0, OpenID Connect, and more.
    * **Local accounts** that enable users to sign up and sign in with a username (or email address or other ID) and password.
* **Keys** - Add and manage encryption keys for signing and validating tokens, client secrets, certificates, and passwords.
  
An **Azure AD B2C tenant** is the **first resource** you need to create to get started with Azure AD B2C. Learn how in Tutorial: [**Create an Azure Active Directory B2C tenant**](azure-acticedirectoryB2C_CreateTenant).

## Accounts in Azure AD B2C
Azure AD B2C defines **several types of user accounts**. Azure Active Directory, Azure Active Directory **B2B**, and Azure Active Directory **B2C** share these account types.

* **Work account** - Users with work accounts can manage resources in a tenant, and with an **administrator role**, can also manage tenants. Users with work accounts can *create new consumer accounts, reset passwords, block/unblock accounts, and set permissions or assign an account to a security group.*
* **Guest account** - External users you invite to your tenant as guests. A typical scenario for inviting a guest user to your Azure AD B2C tenant **is to share administration responsibilities**.
* **Consumer account** - Consumer accounts are the accounts created in your Azure AD B2C directory when **users complete the sign-up user journey** in an application you've registered in your tenant.

![image info](/img/azure/3/portal-01-users.png)

## Consumer accounts
With a consumer account, users can sign in to the applications that you've secured with Azure AD B2C. Users with consumer accounts can't, however, access Azure resources, for example the Azure portal.

A consumer account can be associated with these identity types:

* **Local identity**, with the username and password stored locally in the Azure AD B2C directory. We often refer to these identities as **&quot;local accounts.&quot;**
  
* **Social or enterprise identities**, where the identity of the user is managed by a federated identity provider like Facebook, Microsoft, ADFS, or Salesforce.
  
A user with a consumer account can sign in with multiple identities, for example username, email, employee ID, government ID, and others. A single account can have multiple identities, both local and social.

![image info](/img/azure/3/identities.png)

Azure AD B2C lets you manage common attributes of consumer account profiles like **display name, surname, given name, city, and others**. You can also **extend the Azure AD schema** to store additional information about your users. For example, their **country/region** or **residency**, **preferred language**, and **preferences** like whether they want to **subscribe to a newsletter** or **enable multi-factor authentication**.

## External identity providers

You can configure Azure AD B2C to allow users to sign in to your application with credentials from **external social or enterprise identity providers (IdP)**. Azure AD B2C supports external identity providers like **Facebook, Microsoft account, Google, Twitter, and any identity provider** that supports *OAuth 1.0, OAuth 2.0, OpenID Connect, and SAML* protocols.

![image info](/img/azure/3/external-idps.png)

With external identity provider federation, you can offer your consumers the ability to **sign in with their existing social or enterprise accounts**, *without having to create a new account just for your application.*

On the sign-up or sign-in page, Azure AD B2C presents a list of external identity providers the user can choose for **sign-in**. Once they select one of the external identity providers, they're taken (redirected) to the selected provider's website to complete the sign in process. After the user successfully signs in, they're returned to Azure AD B2C for authentication of the account in your application.

![image info](/img/azure/3/external-idp.png)

## Identity experiences: user flows or custom policies

The extensible policy framework of Azure AD B2C is its core strength. Policies describe your users' identity experiences such as sign up, sign in, and profile editing.

In Azure AD B2C, there are **two primary paths** you can take to provide these identity experiences: **user flows** and **custom policies**.

* **User flows** are predefined, built-in, configurable policies that we provide so you can create **sign-up, sign-in, and policy editing** experiences in minutes.

* **Custom policies** enable you to create your own user journeys for complex identity experience scenarios.

Both user flows and custom policies are powered by the **Identity Experience Framework, Azure AD B2C's policy orchestration engine.**

## User flow
To help you quickly set up the most common identity tasks, the Azure portal includes several predefined and configurable policies called user flows.

You can configure user flow settings like these to control identity experience behaviors in your applications:

  * Account types used for sign-in, such as social accounts like a Facebook, or local accounts that use an email address and password for sign-in
  * Attributes to be collected from the consumer, such as first name, postal code, or country/region of residency
  * Azure Multi-Factor Authentication (MFA)
  * Customization of the user interface
  * Set of claims in a token that your application receives after the user completes the user flow
  * Session management
  * ...and more.
  
Most common identity scenarios for the majority of mobile, web, and single-page applications can be defined and implemented effectively with user flows. We recommend that you use the built-in user flows unless you have complex user journey scenarios that require the full flexibility of custom policies.

## Custom policy
Custom policies unlock access to the full power of the **Identity Experience Framework (IEF) orchestration engine**. With custom policies, you can leverage IEF to build almost any authentication, user registration, or profile editing experience that you can imagine.

The Identity Experience Framework gives you the ability to construct user journeys with any combination of steps. For example:

* Federate with other identity providers
* First- and third-party multi-factor authentication (MFA) challenges
* Collect any user input
* Integrate with external systems using REST API communication

Each such user journey is defined by a policy, and you can build as many or as few policies as you need to enable the best user experience for your organization.

![image info](/img/azure/3/custom-policy.png)

A custom policy is defined by several XML files that refer to each other in a hierarchical chain. The XML elements define the claims schema, claims transformations, content definitions, claims providers, technical profiles, user journey orchestration steps, and other aspects of the identity experience.

The powerful flexibility of custom policies is most appropriate for when you need to build complex identity scenarios. Developers configuring custom policies must define the trusted relationships in careful detail to include metadata endpoints, exact claims exchange definitions, and configure secrets, keys, and certificates as needed by each identity provider.

## Protocols and tokens

* For applications, Azure AD B2C supports the **OAuth 2.0, OpenID Connect, and SAML protocols** for user journeys. Your application starts the user journey by issuing authentication requests to Azure AD B2C. The result of a request to Azure AD B2C is a **security token**, such as an **ID token, access token, or SAML token**. This security token defines the user's identity within the application.

* For external identities, Azure AD B2C supports **federation with any OAuth 1.0, OAuth 2.0, OpenID Connect, and SAML identity providers.**

The following diagram shows how Azure AD B2C can communicate using a variety of protocols within the same authentication flow:

![image info](/img/azure/3/protocols.png)

1. The relying party application initiates an authorization request to Azure AD B2C using OpenID Connect.
2. When a user of the application chooses to sign in using an external identity provider that uses the SAML protocol, Azure AD B2C invokes the SAML protocol to communicate with that identity provider.
3. After the user completes the sign-in operation with the external identity provider, Azure AD B2C then returns the token to the relying party application using OpenID Connect.

## Application integration
When a user wants to sign in to your application, whether it's a web, mobile, desktop, or single-page application (SPA), the application initiates an authorization request to a user flow- or custom policy-provided endpoint. The user flow or custom policy defines and controls the user's experience. When they complete a user flow, for example the sign-up or sign-in flow, Azure AD B2C generates a token, then redirects the user back to your application

![image info](/img/azure/3/app-integration.png)

Multiple applications can use the same user flow or custom policy. A single application can use multiple user flows or custom policies.

**For example**, to sign in to an application, the application uses the sign up or sign in user flow. After the user has signed in, they may want to edit their profile, so the application initiates another authorization request, this time using the profile edit user flow.

## Seamless user experiences
In Azure AD B2C, you can craft your users' identity experiences so that the pages they're shown blend seamlessly with the look and feel of your brand. You get nearly full control of the HTML and CSS content presented to your users when they proceed through your application's identity journeys. With this flexibility, you can maintain brand and visual consistency between your application and Azure AD B2C.

![image info](/img/azure/3/seamless-ux.png)

## Localization
Language customization in Azure AD B2C allows you to accommodate different languages to suit your customer needs. **Microsoft provides the translations for 36 languages**, but you can also provide your own translations for any language. Even if your experience is provided for only a single language, you can customize any text on the pages.

![image info](/img/azure/3/localization.png)

## Add your own business logic

If you choose to use custom policies, you can integrate with a RESTful API in a user journey to add your own business logic to the journey. For example, Azure AD B2C can exchange data with a RESTful service to:

* Display custom user-friendly error messages.
* Validate user input to prevent malformed data from persisting in your user directory. For example, you can modify the data entered by the user, such as capitalizing their first name if they entered it in all lowercase.
* Enrich user data by further integrating with your corporate line-of-business application.
* Using RESTful calls, you can send push notifications, update corporate databases, run a user migration process, manage permissions, audit databases, and more.
  
Loyalty programs are another scenario enabled by Azure AD B2C's support for calling REST APIs. For example, your RESTful service can receive a user's email address, query your customer database, then return the user's loyalty number to Azure AD B2C. The return data can be stored in the user's directory account in Azure AD B2C, then be further evaluated in subsequent steps in the policy, or be included in the access token.

![image info](/img/azure/3/lob-integration.png)

You can add a REST API call at any step in the user journey defined by a custom policy. For example, you can call a REST API:

* During sign-in, just before Azure AD B2C validates the credentials
* Immediately after sign-in
* Before Azure AD B2C creates a new account in the directory
* After Azure AD B2C creates a new account in the directory
* Before Azure AD B2C issues an access token

## Protect customer identities
Azure AD B2C complies with the security, privacy, and other commitments described in the Microsoft Azure Trust Center.

Sessions are modeled as encrypted data, with the decryption key known only to the Azure AD B2C Security Token Service. A strong encryption algorithm, AES-192, is used. All communication paths are protected with TLS for confidentiality and integrity. Our Security Token Service uses an Extended Validation (EV) certificate for TLS. In general, the Security Token Service mitigates cross-site scripting (XSS) attacks by not rendering untrusted input.

![image info](/img/azure/3/user-data.png)

## Access to user data

Azure AD B2C tenants share many characteristics with enterprise Azure Active Directory tenants used for employees and partners. Shared aspects include mechanisms for viewing administrative roles, assigning roles, and auditing activities.

You can assign roles to control who can perform certain administrative actions in Azure AD B2C, including:

* Create and manage all aspects of user flows
* Create and manage the attribute schema available to all user flows
* Configure identity providers for use in direct federation
* Create and manage trust framework policies in the Identity Experience Framework (custom policies)
* Manage secrets for federation and encryption in the Identity Experience Framework (custom policies)
  

## Multi-factor authentication (MFA)

Azure AD B2C multi-factor authentication (MFA) helps safeguard access to data and applications while maintaining simplicity for your users. It provides additional security by requiring a second form of authentication, and delivers strong authentication by offering a range of easy-to-use authentication methods. Your users may or may not be challenged for MFA based on configuration decisions that you can make as an administrator.

## Smart account lockout

To prevent brute-force password guessing attempts, Azure AD B2C uses a sophisticated strategy to lock accounts based on the IP of the request, the passwords entered, and several other factors. The duration of the lockout is automatically increased based on risk and the number of attempts.

![image info](/img/azure/3/smart-lockout1.png)

## Password complexity

During sign up or password reset, your users must supply a password that meets complexity rules. By default, Azure AD B2C enforces a strong password policy. Azure AD B2C also provides configuration options for specifying the complexity requirements of the passwords your customers use.

You can configure password complexity requirements in both **user flows** and **custom policies**.

## Auditing and logs

Azure AD B2C emits audit logs containing activity information about its resources, issued tokens, and administrator access. You can use these audit logs to understand platform activity and diagnose issues. Audit log entries are available soon after the activity that generated the event occurs.

In an audit log, which is available for your Azure AD B2C tenant or for a particular user, you can find information including:

* Activities concerning the authorization of a user to access B2C resources (for example, an administrator accessing a list of B2C policies)
* Activities related to directory attributes retrieved when an administrator signs in using the Azure portal
* Create, read, update, and delete (CRUD) operations on B2C applications
* CRUD operations on keys stored in a B2C key container
* CRUD operations on B2C resources (for example, policies and identity providers)
* Validation of user credentials and token issuance

![image info](/img/azure/3/audit-log.png)

## Usage insights

Azure AD B2C allows you to discover when people sign up or sign in to your web app, where your users are located, and what browsers and operating systems they use. By integrating Azure Application Insights into Azure AD B2C by using custom policies, you can gain insight into how people sign up, sign in, reset their password or edit their profile. With such knowledge, you can make data-driven decisions for your upcoming development cycles.</content><author><name>Girish Godage</name></author><category term="learning" /><summary type="html">What is Azure Active Directory B2C?</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/img/azureAD.png" /></entry><entry><title type="html">Creating A Step-By-Step End-To-End Database Server-Side Blazor Application</title><link href="http://localhost:4000/blog/creating-step-by-step-ServerSideBlazorApp" rel="alternate" type="text/html" title="Creating A Step-By-Step End-To-End Database Server-Side Blazor Application" /><published>2020-03-10T10:26:00+00:00</published><updated>2020-03-10T10:26:00+00:00</updated><id>http://localhost:4000/blog/creating-Step-by-Step-ServerSideBlazorApp</id><content type="html" xml:base="http://localhost:4000/blog/creating-step-by-step-ServerSideBlazorApp">## Creating A Step-By-Step End-To-End Database Server-Side Blazor Application

The primary benefit we have when using **server-side Blazor** is that we do not have to make web http calls from the client code to the server code. This reduces the code we need to write and eliminates many security concerns.

In this article, we will demonstrate how a list of **Weather forecasts** can be added to the database by each user. A user will only have the ability to see their own forecasts.

![image info](/img/blazor/3/1.png)

## Use SQL Server

![image info](/img/blazor/3/2.png)

The new project template in **Visual Studio** will allow you to create a database using [SQL Server Express LocalDB](https://docs.microsoft.com/en-us/aspnet/core/tutorials/first-mvc-app/working-with-sql?view=aspnetcore-2.2&amp;tabs=visual-studio#sql-server-express-localdb). However, it can be problematic to install and configure. Using the free **SQL Server 2019 Developer server** (or the full **SQL Server**) is recommended.

Download and install **SQL Server 2019 Developer Edition** from the following link:

https://www.microsoft.com/en-us/sql-server/sql-server-downloads

## Create The Blazor Application

![image info](/img/blazor/3/3.png)


Open [Visual Studio](https://visualstudio.microsoft.com/).

![image info](/img/blazor/3/4.png)

Select **Create a new Project**.

![image info](/img/blazor/3/5.png)

Select **Blazor App** and click **Next**.

![image info](/img/blazor/3/6.png)

Name it **EndToEnd** and click **Create**.

![image info](/img/blazor/3/7.png)

Select **Blazor Server App**.

![image info](/img/blazor/3/8.png)

Click the Change link under Authentication.

![image info](/img/blazor/3/9.png)

Select **Individual User Accounts** and **Store user accounts in-app.**

![image info](/img/blazor/3/10.png)

Click **Create**.

![image info](/img/blazor/3/11.png)

The **project** will be created.

## Create The Database

![image info](/img/blazor/3/12.png)

Open the **SQL Server Object Explorer**.

![image info](/img/blazor/3/13.png)

Add a connection to your local database server if you don’t already have it in the SQL Server list.

For this tutorial, we do not want to use the SQL Express server on (localdb) that you may already see in the list.

![image info](/img/blazor/3/14.png)

You will specify just the server and **Connect**.

![image info](/img/blazor/3/15.png)

Expand the tree under the local **SQL server**, right-click on the **Databases** folder and select **Add New Database**.

![image info](/img/blazor/3/16.png)

Give the **database** a name and press **Enter**.

The **database** will be created.

![image info](/img/blazor/3/17.png)

**Right-Click** on the **Database** node and select **Properties**.

![image info](/img/blazor/3/18.png)

Open the **Properties** window if it is not already opened.

The **Properties** window for the **database** will display.

**Copy** the **Connection string** for the **database**.

![image info](/img/blazor/3/19.png)

Open the **appsettings.json** file.

![image info](/img/blazor/3/20.png)

**Paste** in the **connection string** for the **DefaultConnection** and **save** the file.

## Run the Application

![image info](/img/blazor/3/21.png)

Hit **F5** to run the application.

![image info](/img/blazor/3/22.png)

The application will open in your **web browser**.

**Click** the **Register** link.

![image info](/img/blazor/3/23.png)

Enter the information to create a **new account**.

Click the **Register** button.

![image info](/img/blazor/3/24.png)

Because this is the first time the database is being used, you will see a message asking you to run the **migration scripts** that will create the database objects needed to support the user membership code.

Click the **Apply Migrations** button.

![image info](/img/blazor/3/25.png)

After the message changes to Migrations Applied, refresh the page in the web browser.

![image info](/img/blazor/3/26.png)

After clicking refresh in your web browser, a popup will require you to click **Continue**.

![image info](/img/blazor/3/27.png)

Click the **Click here to confirm your account link**.

![image info](/img/blazor/3/28.png)

On the **Confirm email** page, click the name of the application to navigate to the home page.

![image info](/img/blazor/3/29.png)

Now you can click the **Log in** link to log in using the account you just created.

![image info](/img/blazor/3/30.png)

You will now be **logged into the application**.

You can click around the application and see that it works.

![image info](/img/blazor/3/31.png)

The **Fetch data** page currently shows random data. We will alter the application to allow us to **add, update, and delete** this data in the database.

**Close** the ***web browser*** to **stop** the application.

## Do Not Require Account Confirmation

![image info](/img/blazor/3/32.png)


If we do not intend to configure **email account verification** (using the directions at this link: https://bit.ly/2tf2ym4), we can open the **Startup.cs** file and change the following line:

```dotnetcli
services.AddDefaultIdentity&lt;IdentityUser&gt;(
                options =&gt; options.SignIn.RequireConfirmedAccount = true)
                .AddEntityFrameworkStores&lt;ApplicationDbContext&gt;();
```

To

```dotnetcli

services.AddDefaultIdentity&lt;IdentityUser&gt;(
                options =&gt; options.SignIn.RequireConfirmedAccount = false)
                .AddEntityFrameworkStores&lt;ApplicationDbContext&gt;();
```

## Create The Database

Create a new **Server Side Blazor** project and *right-click* on the **wwwroot** folder and select **Add** then **New Item…**

![image info](/img/blazor/3/33.png)

In the **SQL Server Object Explorer window**, in **Visual Studio**, we see the **tables** that the ***migration scripts added***.

![image info](/img/blazor/3/34.png)

*Right-click* on the **Tables** node and select **Add New Table**.

![image info](/img/blazor/3/35.png)

Paste the following script in the **T-SQL** window and then click the **Update** button:

```
CREATE TABLE [dbo].[WeatherForecast] (
    [Id]           INT           IDENTITY (1, 1) NOT NULL,
    [Date]         DATETIME      NULL,
    [TemperatureC] INT           NULL,
    [TemperatureF] INT           NULL,
    [Summary]      NVARCHAR (50) NULL,
    [UserName]     NVARCHAR (50) NULL,
    PRIMARY KEY CLUSTERED ([Id] ASC)
);
```
![image info](/img/blazor/3/36.png)

The script will prepare.

Click the **Update Database** button.

![image info](/img/blazor/3/37.png)

Back in the **Server Explorer** window, *right-click* on Tables and select **Refresh**.

![image info](/img/blazor/3/38.png)

The **WeatherForecast** table will display.

*Right-click* on the table and select **Show Table Data**.

![image info](/img/blazor/3/39.png)

We will enter some **sample data** so that we will be able to **test** the data connection in the next steps.

Set the **UserName** field to the **username** of the **user** that we **registered** an account for earlier.

## Create The Data Context

![image info](/img/blazor/3/40.png)

If you do not already have it installed, install EF Core Power Tools from:

https://marketplace.visualstudio.com/items?itemName=ErikEJ.EFCorePowerTools


![image info](/img/blazor/3/41.png)

(Note: Before installing, close **Visual Studio**)

(Note: Please give this project a **5** star review on **marketplace.visualstudio.com**!)

![image info](/img/blazor/3/42.png)

*Right-click* on the **project** node in the **Solution Explorer** and select **EF Core Power** Tools then **Reverse Engineer**.

![image info](/img/blazor/3/43.png)

Click the **Add** button.

![image info](/img/blazor/3/44.png)

*Connect* to the **database**.

Select the **database connection** in the dropdown and click **OK**.

![image info](/img/blazor/3/45.png)

Select the **WeatherForecast** table and click **OK**.

![image info](/img/blazor/3/46.png)

Set the values and click **OK**.

![image info](/img/blazor/3/47.png)

In the **Solution Explorer**, you will see the **Data Context** has been created.

Click the **OK** button to close the popup.

![image info](/img/blazor/3/48.png)

Open the **Startup.cs** file.

Add the following code to the **ConfigureServices** section:

```dotnetcli

// Read the connection string from the appsettings.json file
    // Set the database connection for the EndtoEndContext
    services.AddDbContext&lt;EndToEndDB.Data.EndToEnd.EndtoEndContext&gt;(options =&gt;
    options.UseSqlServer(
        Configuration.GetConnectionString(&quot;DefaultConnection&quot;)));

```

Also, change this line:


```dotnetcli
    
    services.AddSingleton&lt;WeatherForecastService&gt;();
```
           


to this:

```dotnetcli
    
    // Scoped creates an instance for each user
    services.AddScoped&lt;WeatherForecastService&gt;();
```

![image info](/img/blazor/3/49.png)

**Save** the file.

Select **Build**, then **Rebuild Solution**.

## Read From The Database

![image info](/img/blazor/3/50.png)
            
**Delete** the **Data/WeatherForecast.cs** file in the project.

We will use the **Data/EndToEnd/WeatherForcast.cs** class file created by the **EF Core Tools instead**.

![image info](/img/blazor/3/51.png)

Open the **WeatherForecastService.cs** file.

**Replace all the code** with the following code:

```dotnetcli

using EndToEndDB.Data.EndToEnd;
using Microsoft.EntityFrameworkCore;
using System.Collections.Generic;
using System.Linq;
using System.Threading.Tasks;
namespace EndToEnd.Data
{
    public class WeatherForecastService
    {
        private readonly EndtoEndContext _context;
        public WeatherForecastService(EndtoEndContext context)
        {
            _context = context;
        }
        public async Task&lt;List&lt;WeatherForecast&gt;&gt;
            GetForecastAsync(string strCurrentUser)
        {
            // Get Weather Forecasts  
            return await _context.WeatherForecast
                 // Only get entries for the current logged in user
                 .Where(x =&gt; x.UserName == strCurrentUser)
                 // Use AsNoTracking to disable EF change tracking
                 // Use ToListAsync to avoid blocking a thread
                 .AsNoTracking().ToListAsync();
        }
    }
}

```

to this

```dotnetcli

    private readonly EndtoEndContext _context;
    
    public WeatherForecastService(EndtoEndContext context)
        {
            _context = context;
        }

```

Connects to the **database** using the *datacontext* we created with the **EF Core tools**.

![image info](/img/blazor/3/52.png)   

Finally, open the **FetchData.razor** file.

*Replace all the code* with the following code:


```dotnetcli

@page &quot;/fetchdata&quot;
@using EndToEnd.Data
@using EndToEndDB.Data.EndToEnd
@inject AuthenticationStateProvider AuthenticationStateProvider
@*
    Using OwningComponentBase ensures that the service and related services
    that share its scope are disposed with the component.
    Otherwise DbContext in ForecastService will live for the life of the
    connection, which may be problematic if clients stay
    connected for a long time.
    We access WeatherForecastService using @Service
*@
@inherits OwningComponentBase&lt;WeatherForecastService&gt;
&lt;h1&gt;Weather forecast&lt;/h1&gt;
&lt;!-- AuthorizeView allows us to only show sections of the page --&gt;
&lt;!-- based on the security on the current user --&gt;
&lt;AuthorizeView&gt;
    &lt;!-- Show this section if the user is logged in --&gt;
    &lt;Authorized&gt;
        &lt;h4&gt;Hello, @context.User.Identity.Name!&lt;/h4&gt;
        @if (forecasts == null)
        {
            &lt;!-- Show this if the current user has no data... yet... --&gt;
            &lt;p&gt;&lt;em&gt;Loading...&lt;/em&gt;&lt;/p&gt;
        }
        else
        {
            &lt;!-- Show the forecasts for the current user --&gt;
            &lt;table class=&quot;table&quot;&gt;
                &lt;thead&gt;
                    &lt;tr&gt;
                        &lt;th&gt;Date&lt;/th&gt;
                        &lt;th&gt;Temp. (C)&lt;/th&gt;
                        &lt;th&gt;Temp. (F)&lt;/th&gt;
                        &lt;th&gt;Summary&lt;/th&gt;
                    &lt;/tr&gt;
                &lt;/thead&gt;
                &lt;tbody&gt;
                    @foreach (var forecast in forecasts)
                    {
                        &lt;tr&gt;
                            &lt;td&gt;@forecast.Date.Value.ToShortDateString()&lt;/td&gt;
                            &lt;td&gt;@forecast.TemperatureC&lt;/td&gt;
                            &lt;td&gt;@forecast.TemperatureF&lt;/td&gt;
                            &lt;td&gt;@forecast.Summary&lt;/td&gt;
                        &lt;/tr&gt;
                    }
                &lt;/tbody&gt;
            &lt;/table&gt;
        }
    &lt;/Authorized&gt;
    &lt;!-- Show this section if the user is not logged in --&gt;
    &lt;NotAuthorized&gt;
        &lt;p&gt;You're not signed in.&lt;/p&gt;
    &lt;/NotAuthorized&gt;
&lt;/AuthorizeView&gt;
```

```dotnetcli


@code {
    // AuthenticationState is available as a CascadingParameter
    [CascadingParameter]
    private Task&lt;AuthenticationState&gt; authenticationStateTask { get; set; }
    List&lt;WeatherForecast&gt; forecasts;
    protected override async Task OnInitializedAsync()
    {
        // Get the current user
        var user = (await authenticationStateTask).User;
        // Get the forecasts for the current user
        // We access WeatherForecastService using @Service
        forecasts = await @Service.GetForecastAsync(user.Identity.Name);
    }
}

```

This code simply calls the **GetForecastAsync** method we created in the previous step, passing the **username** of the currently logged in user.

In a normal web application we would have to make a *http web call from this client code to the code that connects to the database*.

With server-side **Blazor** we don’t have to do that, yet the call is still secure because the pages are rendered on the server.

![image info](/img/blazor/3/53.png)   

**Build** and run the project.

If we are not logged in, and we go to the **Fetch data** page, we will see a message indicating we are not signed in.

![image info](/img/blazor/3/54.png)   

Click the **Login** button.

![image info](/img/blazor/3/55.png)   

*Log in as the user we created data* for earlier.

![image info](/img/blazor/3/56.png)  

After you are logged in, switch to the **Fetch data** page and you will see the data for the user we entered earlier.

## Inserting Data Into The Database

![image info](/img/blazor/3/57.png)  

Open the **WeatherForecastService.cs** file and add the following method:

```dotnetcli
public Task&lt;WeatherForecast&gt;
            CreateForecastAsync(WeatherForecast objWeatherForecast)
        {
            _context.WeatherForecast.Add(objWeatherForecast);
            _context.SaveChanges();
            return Task.FromResult(objWeatherForecast);
        }

```

![image info](/img/blazor/3/58.png)  

Open the **FetchData.razor** file.

Add the following HTML markup directly under the existing table element:

```dotnetcli
    &lt;p&gt;
        &lt;!-- Add a new forecast --&gt;
        &lt;button class=&quot;btn btn-primary&quot;
                @onclick=&quot;AddNewForecast&quot;&gt;
            Add New Forecast
        &lt;/button&gt;
    &lt;/p&gt;
```

This adds a button to allow a new forecast to be added.

Add the following code below the previous code:

```dotnetcli
    @if (ShowPopup)
        {
            &lt;!-- This is the popup to create or edit a forecast --&gt;
            &lt;div class=&quot;modal&quot; tabindex=&quot;-1&quot; style=&quot;display:block&quot; role=&quot;dialog&quot;&gt;
                &lt;div class=&quot;modal-dialog&quot;&gt;
                    &lt;div class=&quot;modal-content&quot;&gt;
                        &lt;div class=&quot;modal-header&quot;&gt;
                            &lt;h3 class=&quot;modal-title&quot;&gt;Edit Forecast&lt;/h3&gt;
                            &lt;!-- Button to close the popup --&gt;
                            &lt;button type=&quot;button&quot; class=&quot;close&quot;
                                    @onclick=&quot;ClosePopup&quot;&gt;
                                &lt;span aria-hidden=&quot;true&quot;&gt;X&lt;/span&gt;
                            &lt;/button&gt;
                        &lt;/div&gt;
                        &lt;!-- Edit form for the current forecast --&gt;
                        &lt;div class=&quot;modal-body&quot;&gt;
                            &lt;input class=&quot;form-control&quot; type=&quot;text&quot;
                                    placeholder=&quot;Celsius forecast&quot;
                                    @bind=&quot;objWeatherForecast.TemperatureC&quot; /&gt;
                            &lt;input class=&quot;form-control&quot; type=&quot;text&quot;
                                    placeholder=&quot;Fahrenheit forecast&quot;
                                    @bind=&quot;objWeatherForecast.TemperatureF&quot; /&gt;
                            &lt;input class=&quot;form-control&quot; type=&quot;text&quot;
                                    placeholder=&quot;Summary&quot;
                                    @bind=&quot;objWeatherForecast.Summary&quot; /&gt;
                            &lt;br /&gt;
                            &lt;!-- Button to save the forecast --&gt;
                            &lt;button class=&quot;btn btn-primary&quot;
                                    @onclick=&quot;SaveForecast&quot;&gt;
                                Save
                            &lt;/button&gt;
                        &lt;/div&gt;
                    &lt;/div&gt;
                &lt;/div&gt;
            &lt;/div&gt;
        }
```

This adds a form (that will be displayed a popup because the class for the DIV is modal), that allows the user to enter (and later edit) data for a forecast.

We do not need JavaScript to make this **popup** show. We only need to wrap this code with:

```dotnetcli
     @if (ShowPopup)
        {
            ...
        }
```

           


When the **ShowPopup** value is true the **popup** will show. When the value is false, the **popup** will disappear.

Add the following code to the *@code* section:


```dotnetcli


    WeatherForecast objWeatherForecast = new WeatherForecast();
    bool ShowPopup = false;
    void ClosePopup()
    {
        // Close the Popup
        ShowPopup = false;
    }
    void AddNewForecast()
    {
        // Make new forecast
        objWeatherForecast = new WeatherForecast();
        // Set Id to 0 so we know it is a new record
        objWeatherForecast.Id = 0;
        // Open the Popup
        ShowPopup = true;
    }
    async Task SaveForecast()
    {
        // Close the Popup
        ShowPopup = false;
        // Get the current user
        var user = (await authenticationStateTask).User;
        // A new forecast will have the Id set to 0
        if (objWeatherForecast.Id == 0)
        {
            // Create new forecast
            WeatherForecast objNewWeatherForecast = new WeatherForecast();
            objNewWeatherForecast.Date = System.DateTime.Now;
            objNewWeatherForecast.Summary = objWeatherForecast.Summary;
            objNewWeatherForecast.TemperatureC =
            Convert.ToInt32(objWeatherForecast.TemperatureC);
            objNewWeatherForecast.TemperatureF =
            Convert.ToInt32(objWeatherForecast.TemperatureF);
            objNewWeatherForecast.UserName = user.Identity.Name;
            // Save the result
            var result =
            @Service.CreateForecastAsync(objNewWeatherForecast);
        }
        else
        {
            // This is an update
        }
        // Get the forecasts for the current user
        forecasts =
        await @Service.GetForecastAsync(user.Identity.Name);
    }
```

![image info](/img/blazor/3/59.png)  

When you run the project, you can click the **Add New Forecast** button to add an entry.

![image info](/img/blazor/3/60.png)  

The form only requires a **Fahrenheit, Celsius, and a summary**, because the other values (*date and username*), will be set by the code.

![image info](/img/blazor/3/61.png)

After clicking the **Save** button, the entry is saved to the database and displayed.

## Updating The Data

![image info](/img/blazor/3/62.png)

Open the **WeatherForecastService.cs** file and **add** the following method:

```dotnetcli
     public Task&lt;bool&gt;
            UpdateForecastAsync(WeatherForecast objWeatherForecast)
        {
            var ExistingWeatherForecast =
                _context.WeatherForecast
                .Where(x =&gt; x.Id == objWeatherForecast.Id)
                .FirstOrDefault();
            if (ExistingWeatherForecast != null)
            {
                ExistingWeatherForecast.Date =
                    objWeatherForecast.Date;
                ExistingWeatherForecast.Summary =
                    objWeatherForecast.Summary;
                ExistingWeatherForecast.TemperatureC =
                    objWeatherForecast.TemperatureC;
                ExistingWeatherForecast.TemperatureF =
                    objWeatherForecast.TemperatureF;
                _context.SaveChanges();
            }
            else
            {
                return Task.FromResult(false);
            }
            return Task.FromResult(true);
        }
```

![image info](/img/blazor/3/63.png)

Open the **FetchData.razor** file.

**Replace** the existing **table** element with the following markup that adds an edit button in the last column (that calls the **EditForecast** method we will add in the next step):

```dotnetcli

    &lt;table class=&quot;table&quot;&gt;
        &lt;thead&gt;
            &lt;tr&gt;
                &lt;th&gt;Date&lt;/th&gt;
                &lt;th&gt;Temp. (C)&lt;/th&gt;
                &lt;th&gt;Temp. (F)&lt;/th&gt;
                &lt;th&gt;Summary&lt;/th&gt;
                &lt;th&gt;&lt;/th&gt;
            &lt;/tr&gt;
        &lt;/thead&gt;
        &lt;tbody&gt;
            @foreach (var forecast in forecasts)
            {
                &lt;tr&gt;
                    &lt;td&gt;@forecast.Date.Value.ToShortDateString()&lt;/td&gt;
                    &lt;td&gt;@forecast.TemperatureC&lt;/td&gt;
                    &lt;td&gt;@forecast.TemperatureF&lt;/td&gt;
                    &lt;td&gt;@forecast.Summary&lt;/td&gt;
                    &lt;td&gt;
                        &lt;!-- Edit the current forecast --&gt;
                        &lt;button class=&quot;btn btn-primary&quot;
                                @onclick=&quot;(() =&gt; EditForecast(forecast))&quot;&gt;
                            Edit
                        &lt;/button&gt;
                    &lt;/td&gt;
                &lt;/tr&gt;
            }
        &lt;/tbody&gt;
    &lt;/table&gt;



```

Add the following code to the *@code* section:

```dotnetcli
    void EditForecast(WeatherForecast weatherForecast)
    {
        // Set the selected forecast
        // as the current forecast
        objWeatherForecast = weatherForecast;
        // Open the Popup
        ShowPopup = true;
    }
```

This sets the current record to the **objWeatherForecast** object that the **popup** is bound to, and opens the **popup**.

Finally, change the existing **SaveForecast** method to the following:

```dotnetcli
    
    async Task SaveForecast()
    {
        // Close the Popup
        ShowPopup = false;
        // Get the current user
        var user = (await authenticationStateTask).User;
        // A new forecast will have the Id set to 0
        if (objWeatherForecast.Id == 0)
        {
            // Create new forecast
            WeatherForecast objNewWeatherForecast = new WeatherForecast();
            objNewWeatherForecast.Date = System.DateTime.Now;
            objNewWeatherForecast.Summary = objWeatherForecast.Summary;
            objNewWeatherForecast.TemperatureC =
            Convert.ToInt32(objWeatherForecast.TemperatureC);
            objNewWeatherForecast.TemperatureF =
            Convert.ToInt32(objWeatherForecast.TemperatureF);
            objNewWeatherForecast.UserName = user.Identity.Name;
            // Save the result
            var result =
            @Service.CreateForecastAsync(objNewWeatherForecast);
        }
        else
        {
            // This is an update
            var result =
            @Service.UpdateForecastAsync(objWeatherForecast);
        }
        // Get the forecasts for the current user
        forecasts =
        await @Service.GetForecastAsync(user.Identity.Name);
    }



```

This simply adds one line:

```dotnetcli
    // This is an update
    var result =  @Service.UpdateForecastAsync(objWeatherForecast);
```

To the existing method to handle an update rather than an insert.            

![image info](/img/blazor/3/64.png)


When we run the application, we now have an **Edit button** to edit the existing record.


![image info](/img/blazor/3/65.png)

The existing record will display in the **popup**, allowing us to *edit the data and save it*.

![image info](/img/blazor/3/66.png)

The updated record is then displayed in the table.

## Deleting The Data

![image info](/img/blazor/3/67c.png)

Open the **WeatherForecastService.cs** file and **add** the following method:

```dotnetcli

    public Task&lt;bool&gt;
            DeleteForecastAsync(WeatherForecast objWeatherForecast)
    {
        var ExistingWeatherForecast =
            _context.WeatherForecast
            .Where(x =&gt; x.Id == objWeatherForecast.Id)
            .FirstOrDefault();
        if (ExistingWeatherForecast != null)
        {
            _context.WeatherForecast.Remove(ExistingWeatherForecast);
            _context.SaveChanges();
        }
        else
        {
            return Task.FromResult(false);
        }
        return Task.FromResult(true);
    }

```

![image info](/img/blazor/3/68.png)

Open the **FetchData.razor** file.

Add code markup for a **Delete** button under the code markup for the current **Save** button in the **popup**:

```dotnetcli

    &lt;!-- Only show delete button if not a new record --&gt;
        @if (objWeatherForecast.Id &gt; 0)
        {
            &lt;!-- Button to delete the forecast --&gt;
            &lt;button class=&quot;btn btn-primary&quot;
                    @onclick=&quot;DeleteForecast&quot;&gt;
                Delete
            &lt;/button&gt;
        }
```
Add the following code to the *@code* section:

```dotnetcli

    async Task DeleteForecast()
    {
        // Close the Popup
        ShowPopup = false;
        // Get the current user
        var user = (await authenticationStateTask).User;
        // Delete the forecast
        var result = @Service.DeleteForecastAsync(objWeatherForecast);
        // Get the forecasts for the current user
        forecasts =
        await @Service.GetForecastAsync(user.Identity.Name);
    }

```

![image info](/img/blazor/3/69.png)


When we run the code and click the **Edit** button…

![image info](/img/blazor/3/70.png)

… we now see a **Delete** button that will *delete* the record.

![image info](/img/blazor/3/71.png)

However, when we click the **Add New Forecast** button that opens the same **popup**…

![image info](/img/blazor/3/72.png)

The delete button does not display (because there is no record to delete at this point).

## Use Code Behind Method Using Partial Class Support

![image info](/img/blazor/3/73.png)

We can code our **.razor** pages using an *@code* block with **HTML markup** in a single file, or with the **C#** code placed in a *code-behind* file defined as a **partial class**.

Create a new class called **FetchData.razor.cs** using the following code:

```dotnetcli

using EndToEndDB.Data.EndToEnd;
using Microsoft.AspNetCore.Components;
using Microsoft.AspNetCore.Components.Authorization;
using System;
using System.Collections.Generic;
using System.Threading.Tasks;
namespace EndToEnd.Pages
{
    public partial class FetchData
    {
        // AuthenticationState is available as a CascadingParameter
        [CascadingParameter]
        private Task&lt;AuthenticationState&gt; authenticationStateTask { get; set; }
        List&lt;WeatherForecast&gt; forecasts;
        protected override async Task OnInitializedAsync()
        {
            // Get the current user
            var user = (await authenticationStateTask).User;
            // Get the forecasts for the current user
            // We access WeatherForecastService using @Service
            forecasts = await @Service.GetForecastAsync(user.Identity.Name);
        }
        WeatherForecast objWeatherForecast = new WeatherForecast();
        bool ShowPopup = false;
        void ClosePopup()
        {
            // Close the Popup
            ShowPopup = false;
        }
        void AddNewForecast()
        {
            // Make new forecast
            objWeatherForecast = new WeatherForecast();
            // Set Id to 0 so we know it is a new record
            objWeatherForecast.Id = 0;
            // Open the Popup
            ShowPopup = true;
        }
        async Task SaveForecast()
        {
            // Close the Popup
            ShowPopup = false;
            // Get the current user
            var user = (await authenticationStateTask).User;
            // A new forecast will have the Id set to 0
            if (objWeatherForecast.Id == 0)
            {
                // Create new forecast
                WeatherForecast objNewWeatherForecast = new WeatherForecast();
                objNewWeatherForecast.Date = System.DateTime.Now;
                objNewWeatherForecast.Summary = objWeatherForecast.Summary;
                objNewWeatherForecast.TemperatureC =
                Convert.ToInt32(objWeatherForecast.TemperatureC);
                objNewWeatherForecast.TemperatureF =
                Convert.ToInt32(objWeatherForecast.TemperatureF);
                objNewWeatherForecast.UserName = user.Identity.Name;
                // Save the result
                var result =
                @Service.CreateForecastAsync(objNewWeatherForecast);
            }
            else
            {
                // This is an update
                var result =
                @Service.UpdateForecastAsync(objWeatherForecast);
            }
            // Get the forecasts for the current user
            forecasts =
            await @Service.GetForecastAsync(user.Identity.Name);
        }
        void EditForecast(WeatherForecast weatherForecast)
        {
            // Set the selected forecast
            // as the current forecast
            objWeatherForecast = weatherForecast;
            // Open the Popup
            ShowPopup = true;
        }
        async Task DeleteForecast()
        {
            // Close the Popup
            ShowPopup = false;
            // Get the current user
            var user = (await authenticationStateTask).User;
            // Delete the forecast
            var result = @Service.DeleteForecastAsync(objWeatherForecast);
            // Get the forecasts for the current user
            forecasts =
            await @Service.GetForecastAsync(user.Identity.Name);
        }
    }
}
```
Remove the entire *@code* bock from the **FetchData.razor** page.

The application will still behave the same.

## Links

[Blazor.net](https://blazor.net/)

[ASP.NET Core Blazor authentication and authorization](https://docs.microsoft.com/en-us/aspnet/core/security/blazor/?view=aspnetcore-3.0&amp;tabs=visual-studio)

[EF Core Power Tools](https://marketplace.visualstudio.com/items?itemName=ErikEJ.EFCorePowerTools)

[OwningComponentBase (background)](https://github.com/aspnet/AspNetCore/issues/5496)</content><author><name>Girish Godage</name></author><category term="learning" /><summary type="html">Creating A Step-By-Step End-To-End Database Server-Side Blazor Application</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/img/blazor.png" /></entry><entry><title type="html">AZURE DEVOPS – MANAGE YOUR APPLICATION LIFECYCLE IN CLOUD</title><link href="http://localhost:4000/blog/Azure-devops" rel="alternate" type="text/html" title="AZURE DEVOPS – MANAGE YOUR APPLICATION LIFECYCLE IN CLOUD" /><published>2020-03-10T09:21:00+00:00</published><updated>2020-03-10T09:21:00+00:00</updated><id>http://localhost:4000/blog/Azure-devops</id><content type="html" xml:base="http://localhost:4000/blog/Azure-devops">## AZURE DEVOPS – MANAGE YOUR APPLICATION LIFECYCLE IN CLOUD

**Application lifecycle management (ALM)** is the product lifecycle management of computer programs.

It encompasses **requirements management**, **software architecture**, **computer programming**, **software testing**, **software maintenance**, **change management**, **continuous integration**, **project management**, and **release management**.

An **ALM tool** should have the capability to maintain all the aspects of a software lifecycle, such as &lt;span style=&quot;background-color: #2197CB&quot;&gt;**capturing the ideas, users requirement, planning of work, maintaining source code, deploying code using continuous integration and continuous delivery (CI/CD)**&lt;/span&gt;. It should also provide real time project insights to the key stake holders of a project. We can think of an ALM software as one stop shop for a software project/product management as a whole.

In this post we will take a look at **Microsoft’s Azure DevOps offering**, it’s capability as an ALM solution and will see how you can use it for your ALM journey.

## What is Azure DevOps?

In a single sentence, Azure DevOps is a suite of offering in the cloud that you need to **build your software product/project from beginning to end**.

**Azure DevOps**, formerly known as Visual Studio Team System (VSTS). It helps you to plan your project, manage source code in repositories like *Git, TFVC, Subversion, GitHub and deploys the code through the CI/CD pipeline system to cloud or in on-premise resources*. 

Additionally it gives a **collaborative platform** for **developers, business users and test engineers, project managers** under one umbrella to **real time tracking of work** and **quick shipment** of the product.

## Six pillars of Azure DevOps

Azure DevOps has six major pillars which helped it to shape as complete ALM offering in the the cloud. Let’s briefly look into the the major pillars of Azure DevOps

![Image](/img/devops/2/AzureBoard.png &quot;Azure Boards&quot;)**Azure Boards** is a place to **manage and plan** all of your work.** All ideas** and **business requirements** for your software can be **captured here**. You can create **epics**, **features**, **user stories** and then **estimate them** and **plan capacity** of your *team members* and start the **sprint**.

Additionally you can have a real time view of the work item progression in **Kanban boards**, **sprint burn down charts**. 

![Image](/img/devops/2/AzureRepos.png &quot;Azure Repos&quot;)**Azure Repos**  supports various source control system like *Azure Git, Public GitHub, GitHub Enterprise,  Microsoft’s own Team foundation version control (TFVC), External Git*. 

You can choose any one of the repositories for your choice for software development and source code. Once development is done developers can create a pull request with their changes, send it for code review and take part of a collaborative development environment.

![Image](/img/devops/2/AzurePipeline.png &quot;Azure Pipeline&quot;)**Azure Pipelines**  are used for **continuous integration** and **continuous delivery  (CI/CD)** of your code. It allows you to **ship your code faster**. 

Once your code is **developed and committed** to the repos, a **build** gets triggered in the **build pipeline**.&lt;span style=&quot;background-color: #2197CB&quot;&gt; This build pipeline gets the **latest code from the repos**, builds them in the build agent. On a successful build, the *build artifacts gets published using Release pipeline in your targeted deployment environment*.&lt;/span&gt; 

![Image](/img/devops/2/AzureTestPlans.png &quot;Azure Test Plans&quot;)**Azure Test Plans**  is for **planning** and **executing** your **manual**, **automated**  and l**oad test cases**. Once the *build is succeeded test engineers can run a set of tests on the build for verification and validation of the desired functionality*. And they can raise and log any defects or observation on failure  of any of the planned test scenarios in the Azure Boards.

![Image](/img/devops/2/AzureArtifacts.png &quot;Azure Artifacts&quot;)**Azure Artifacts**  manages your private NuGet, npm, Maven packages in a private feed. You can integrate this feed in your favourite IDE such as Visual Studio or Visual Studio Code and restore the packages from this feed while development. Additionally you can integrate this feed in the build pipeline and restore packages from this private repository. **Azure Artifacts** helps a lot where there are a lot of shared common packages in use in the application and you want to control standards, version of those cross cutting atrifacts.

![Image](/img/devops/2/AzureOverview.png &quot;Azure Overview&quot;)**Azure Overview**  gives real time insights of the project  **such as teams velocity**, **CI/CD** results, **number of defects raised vs solved** at any point of time to name a few. **Executive dashboards** can be created with inbuilt capability of it. These dashboard helps greatly for **executive status reporting with risk and quality matrices**.

**Additionally** it supports creating **Wiki pages for the project**. Wiki pages can be used to **create** any **documents/ manual** which are **relevant** for the **ongoing or a completed project**.

## ALM with Azure DevOps in action

Now that we have fair **understanding of six pillars** of Azure DevOps, now lets see how all of these helps in ALM with an example. refer to the Infographic of  this post. This describes ALM in action with Azure DevOps very briefly.

![Image](/img/devops/2/Azure-DevOps-ALM.png &quot;Azure DevOps ALM&quot;)

In the beginning of the project or product it starts with an idea or some business requirement and after going through  multiple steps it takes shape of a usable product. In this example infographic 

- **1.** Customer comes with requirement.
- **2.** Business analysts/ Product owner analyses the requirement and starts documenting them in the Azure Boards. They starts **creating Epic, Feature and user stories**.
- **3.** Engineering team then starts **estimating** the work and **plan** those user stories for sprint.
    - **3.1.** In **parallel** test engineers starts for **Test planning** in Test Plans based on the acceptance criteria of the **user stories**.
  
- **4.** Once sprint kicked in – developers starts picking user stories from the **backlogs** created at step 2 and **starts implementing them**.
- **5.** Once developers are done with the development they creates a **pull request** and sends for **code review**. Upon satisfaction of the code reviewer, developer commits the code in the Azure Repos.
- **6.** A build gets triggered in the **Build Pipeline** as soon as the **code gets committed** in the Azure Repos using continuous integration (CI).&lt;span style=&quot;background-color: #2197CB&quot;&gt; A CI build gets the latest code form the repos, builds them in the build agents, restores any private packages from the feed setup in the Azure Artifacts. It runs unit tests, generates code coverage result and finally generates the build output for deployment.&lt;/span&gt; If any one of the steps configured in the build pipeline fails then whole commit gets rejected and developers gets a build failure notification. Developer fixes the build and commits one more time and CI build triggers automatically.
- **7.** Build output needs to be moved to the deployment environment. In a **Release Pipeline** source of the build artifacts is the output of the CI build of previous step 6, destination will be the servers where these artifacts needs to be deployed. In this example it is deploying the artifacts to the Azure Cloud App Services and Azure SQL databases for a TESTING environment (step 7.1). A gate or approval mechanism can be set before the deployment happens, this gives control to the approvers to pick and choose which build to be deploy.
- **8.** Now that build is deployed in TESTING environment, test engineers starts testing and verifying the build quality. They run a set of manual, automated and load tests as planned during the planning phase (step 3.1) in Azure Test Plans. If they observe any defects then they log then in Azure Boards backlog and creates a new bug/issue/observation. Developer picks these defects once these are planned and fed back to the sprint, fixes the defects and commits to the Azure Repos which eventually triggers CI/CD in Azure Pipelines.
- **9.** If test engineers are satisfied with the build quality and functionality delivered, then this build gets promoted to the upper environment (step 9.1). In this example it is UAT environment for business users to test.
- **10.** Business users now gets a new build in UAT environment with all the features planned for the current sprint. Business user starts verifying the build and functionality in UAT environment.
- **11.** If business users finds any defects then they log them into the Azure Boards backlogs as **bug/observation**. Development and build cycle continues once these defects are planned and fed back to the sprint.
- **12.** If business users are satisfied with the functionality and the build quality then the build gets promoted to the **PRODUCTION** environment (**step 12.1**)
  
Once this is released to **production**, end users starts using the system. If any issue occurs in the production then it again gets logged in the Azure Boards backlog as an **Incident/ Bug/ Issue**.

Like this **planning**, **development** and **build cycle** continues towards shipment of the product.  In the whole process key **stakeholders** of the project keeps an eye on **Azure Overview dashboards** to identify and mitigate any risks.

## Wrap up

Okay, so we are at the end of this post. In this post we have covered six pillars of the Azure Dev Ops and how this can be leveraged to manage your applications life cycle. Azure DevOps is a browser based application in public cloud with first class support and integration with numerous opensource languages and platforms. You can manage your ALM in peace of mind end to end in Azure DevOps.

* If you want to know more about the pricing check [this](https://azure.microsoft.com/en-gb/pricing/details/devops/azure-devops-services/) out.
* To learn more about Azure DevOps check [this](https://www.youtube.com/watch?v=kn2f5a2Z_n4) video.

Oh.. wait, In the meanwhile, you can get started with your free account in Azure DevOps and start exploring. Just log in to https://dev.azure.com with your Microsoft account and start your ALM journey in cloud with Azure DevOps.</content><author><name>Girish Godage</name></author><category term="learning" /><summary type="html">AZURE DEVOPS – MANAGE YOUR APPLICATION LIFECYCLE IN CLOUD</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/img/dev-ops.png" /></entry><entry><title type="html">Amazon pinpoint - Customer Engagement</title><link href="http://localhost:4000/blog/customer-engagement-with-Amazon-pinpoint" rel="alternate" type="text/html" title="Amazon pinpoint - Customer Engagement" /><published>2020-02-02T11:41:00+00:00</published><updated>2020-02-02T11:41:00+00:00</updated><id>http://localhost:4000/blog/customer-engagement-with-Amazon-pinpoint</id><content type="html" xml:base="http://localhost:4000/blog/customer-engagement-with-Amazon-pinpoint">## What is Customer Engagement?

*Customer engagement* refers to the practices that organizations employ to keep users coming back to their applications. Maximizing customer engagement is vital in today's digital marketplace, where an ever-increasing number of brands are competing for a finite amount of customer attention. In this article, we discuss the factors that impact customer engagement, and provide information about specific steps you can take to improve customer engagement.

### Why customer engagement matters

In 2012, the average adult in the United States spent one hour per day using his or her mobile device; by 2017, that figure had increased to 5.6 hours per day. Over half of that time is spent using third-party apps, but the majority of app users access 20 or fewer apps each month. Of those 20 apps, most users spent the vast majority of their time in their top 10 applications.

At the same time, first impressions are everything in today's digital marketplace. Almost a quarter of users have downloaded an app, ran it once, and never opened it again. Additional research suggests that as many as 80% of all app users stop using a given app within 90 days.

These statistics paint a very clear picture: in order to succeed in this highly competitive environment, organizations must deliver digital content that delights and engages their customers.

### Customer engagement in action
Let's assume your organization produces a mobile app for sports fans called All Things Sports. In your app, users can get the latest sports news, purchase memorabilia and event tickets, and play exciting mini-games. Your organization has devoted a large amount of time, effort, and money into creating a best-in-class app and acquiring new users. For this reason, it's essential that your users keep coming back to your application so that your business can grow.

For all applications, user interactions fall into three categories: activation, retention, and conversion. All Things Sports requires users to confirm their email address and select their favorite sports teams; the percentage of users who complete this process is the activation rate. Next, you need to offer content that is compelling enough to keep your users coming back. All Things Sports offers fun mini-games and special offers that encourage users to return to the app often. The likelihood that your customers will return to your application repeatedly is referred to as your retention rate. Finally, your organization needs to make money in order to grow. The main revenue generating activity in All Things Sports is the sale of event tickets and merchandise. The percentage of users who purchase these products through your app is your conversion rate. As the application developer, you have to continuously improve all three of these metrics in order to grow your business.

The first step toward improving these metrics is to gather customer analytics data. Customer analytics data is the data you collect and analyze to better understand the ways in which customers use your application. A thorough analysis of this data can help you better serve your users, thereby increasing their engagement with your application. Making optimizations to your application based on this data can lead to positive reviews, better app store rankings, and increased word of mouth. These factors can increase your activation rate, which can in turn increase your retention and conversion rates. You can start gathering this information by integrating the AWS Mobile SDK into your application. For more information about customer analytics, see our companion article, What are Customer Analytics?

The next step toward improving customer engagement is to create targeted segments of customers based on their characteristics. By targeting specific customers based on their demographics, preferences, behaviors, and interests, you can ensure that you're using the right channels to send the right messages to the right customers. **Amazon Pinpoint** can import app usage data obtained from the AWS Mobile SDK and use it to create customer segments. To learn more about the ways in which you can create customer segments, see our companion article, What is Customer Segmentation?

The final step toward improving customer engagement is to deploy personalized, multi-channel messaging to your customers. You can use these messages to proactively update engaged customers on exciting new features and improvements in your application, or to incentivize less-engaged customers to sign back in to your application. You can also use this messaging to convey vital transactional content—such as password reset emails or purchase confirmations—to all customers. You can also use **Amazon Pinpoint** to meet this need. For more information about multi-channel messaging, see the companion article titled What is Multi-Channel Messaging?

## What Is Amazon Pinpoint?

Amazon Pinpoint is an AWS service that you can use to engage with your customers across multiple messaging channels. You can use **Amazon Pinpoint to send push notifications, emails, SMS text messages, and voice messages**.

## Amazon Pinpoint Features
This section describes the major features of Amazon Pinpoint and the tasks that you can perform by using them.

### Define Audience Segments
Reach the right audience for your messages by defining audience segments. A segment designates which users receive the messages that are sent from a campaign or journey. You can define dynamic segments based on data that's reported by your application, such as operating system or mobile device type. You can also import static segments that you define outside of Amazon Pinpoint.

### Engage Your Audience with Messaging Campaigns
Engage your audience by creating a messaging campaign. A campaign sends tailored messages on a schedule that you define. You can create campaigns that send push notifications, email, SMS text messages, and voice messages.

To experiment with alternative campaign strategies, set up your campaign as an A/B test, and analyze the results with Amazon Pinpoint analytics.

### Create User Journeys
Create custom, multi-step experiences for your customers by designing and building journeys. With journeys, you can send messages to your customers based on their attributes, behaviors, and activities. When you build a journey, you design an automated workflow of activities that perform a variety of different actions—for example, sending an email message to participants, waiting for a certain period of time, or splitting participants based on actions that they take, such as clicking a link in a message.

### Provide Consistent Messaging with Templates
Design consistent messages and reuse content more effectively by creating and using message templates. A message template contains content and settings that you want to reuse in messages that you send for any of your Amazon Pinpoint projects. You can use message templates in email messages, push notifications, SMS messages, and voice messages.

### Deliver Personalized Content
Send content that's customized for each recipient of a message. Using message variables and attributes, you can deliver dynamic, personalized content in messages that you send from campaigns and journeys.

To streamline development, you can also use message variables and attributes to add personalized content to message templates. With message templates, this content can come from attributes that you create directly in Amazon Pinpoint or a machine learning model that you create in Amazon Personalize. By connecting message templates to models in Amazon Personalize, you can use machine learning to send relevant promotions or recommendations to each recipient of a message.

### Analyze User Behavior
Gain insight into your audience and the effectiveness of your campaigns and messaging activities by using the analytics that Amazon Pinpoint provides. You can view trends in your users' level of engagement, purchase activity, demographics, and more. You can also monitor your message traffic by viewing metrics such as the total number of messages that you sent for a campaign or project. Through the Amazon Pinpoint API, your application can also report custom data, which Amazon Pinpoint makes available for analysis.

To analyze or store analytics data outside Amazon Pinpoint, configure Amazon Pinpoint to stream the data to Amazon Kinesis.

### Send Test Messages
Test the design and deliverability of your messages by sending test messages before you send messages to your customers.

## Get Started
Get started with Amazon Pinpoint by **creating a new project** or **completing a tutorial**.

## Getting Started with Amazon Pinpoint

To start sending targeted messages in Amazon Pinpoint, you have to complete a few steps. *For example, you have to add customer contact information into Amazon Pinpoint, and then create segments that target certain customers. Next, you have to create your messages and schedule your campaigns. Finally, after you send your campaigns, you can use the analytics dashboards that are built into Amazon Pinpoint to see how well the campaigns performed.*

### Intended Audience

This tutorial is designed for **marketing and business users**.

### Features Used

This tutorial shows you how to complete all of the following steps by using the Amazon Pinpoint console:

*   Importing customer data from a file.

*   Creating a segment that targets specific users based on their attributes.

*   Creating an email campaign and scheduling it to be sent at a specific time.

*   Viewing email delivery and response data by using the analytics dashboards that are built into Amazon Pinpoint.


### Step 1: Create and Configure a Project

To create a project and verify an email address

1. Open the Amazon Pinpoint console at https://console.aws.amazon.com/pinpoint/.

2. On the **All projects page**, choose **Create a project**.

3. On the **Create a project** window, for **Project name**, enter a name for your project, and then **choose Create**.

**Note**
*The project name can contain up to 64 characters.*

4. On the **Configure features** page, next to **Email**, choose **Configure**.

5. For Email address, type an email address that you want to use to send email. For example, you can use your personal email address, or your work email address. Choose **Verify**.

6. Wait for 1–2 minutes, and then check the inbox for the email address that you specified in step 4. You should see an email from Amazon Web Services (no-reply-aws@amazon.com) with the subject line &quot;Amazon Web Services – Email Address Verification Request in region **RegionName**&quot;, where **RegionName** is the name of the AWS Region that you're configuring Amazon Pinpoint in.

7. Open the email, and then click the link in the body of the email.

8. Return to the Amazon Pinpoint console in your browser. On the Set up email page, choose Save.

Your account is now ready to send email from the email address that you verified. You can add additional email addresses later.

You can also verify entire domains. When you verify a domain, you can send email from any address on that domain. For more information, see Verifying a Domain.



### Step 2: Import Customer Data and Create a Segment

A segment is a group of your customers that share certain attributes. *For example, a segment might contain all of your customers who use version 2.0 of your app on an Android device, or all customers who live in the city of Los Angeles.*

When you create a campaign, you have to choose a segment to send the campaign to. You can send multiple campaigns to a single segment, and you can send a single campaign to multiple segments.

There are two types of segments that you can create in Amazon Pinpoint:

* **Dynamic segments** – Segments that are based on attributes that you define. Dynamic segments can change over time. For example, if you add new endpoints to Amazon Pinpoint, or if you modify or delete existing endpoints, the number of endpoints in that segment may increase or decrease.

* **Imported segments** – Segments that are created outside of Amazon Pinpoint and saved in CSV or JSON format. Imported segments are static—that is, they never change. When you create a new segment, you can use an imported segment as a base segment, and then refine it by adding filters.

In this tutorial, you create an imported segment by uploading a file from your computer. Next, you create a dynamic segment that is based upon the imported segment.

#### Step 2.1: Download and Modify the Sample File

In this section, you download a file that contains fictitious customer data. You also modify the data to include your own contact information. Later in this tutorial, you use this data to create a segment.


1. In a web browser, download the sample file from https://raw.githubusercontent.com/awsdocs/amazon-pinpoint-user-guide/master/examples/Pinpoint_Sample_Import.csv. Save the file to your computer.

  ***Tip**
    You can quickly save this file to your computer by right-clicking the link, and then choosing Save Link As.*

2. Open the file in a text editor or spreadsheet application. On the last row of the file, replace the items in angle brackets (&lt;…&gt;) with your own contact information.

    In the Address column, provide the same email address that you verified in Step 1.

    In the User.UserAttributes.Company column, specify a company name that's different from the fictitious company names in the file. You'll use this unique company name when you define the criteria for your targeted segment in the next section.

    **Note**
    *You don't have to provide your information for each column in the file. However, at a minimum, you have to provide information for the ChannelType, Address, and User.UserAttributes.Company columns.*

    The email that you create later in this tutorial uses several of these fields to create a personalized message.

3. When you finish, save the file.

    **Note**
    *If you used a spreadsheet application to modify the file, make sure that you save the modified file in Comma-Separated Values (.csv) format. Amazon Pinpoint can only import .csv and .json files.*

#### Step 2.2: Import a File that Contains Customer Data

Now that you have a file that contains customer data, you can import it into Amazon Pinpoint. To import customer data, you have to create a new segment.

**To create an imported segment**

1. In the Amazon Pinpoint console, in the navigation pane, choose **Segments**.

2. Choose **Create a segment**.

3. On the **Create a segment** page, choose **Import a segment**.

4. In the **Specifications** section, under **Import method**, choose **Upload files from your computer**.

5. Select **Choose files**. Navigate to the Pinpoint_Sample_Import.csv file that you downloaded and modified in the previous section.

6. Choose **Create segment**. Amazon Pinpoint copies the file from your computer and creates a segment. Wait for about 1 minute while the import completes.

#### Step 2.3: Create a Targeted Segment
Your Amazon Pinpoint project now contains some customer data, as well as a segment that contains your entire customer list. It also contains your contact information.

In this section, you create a targeted segment. You add segment criteria that filter the segment so that you're the only member of the segment.

**To create the segment**

1. On the **Segments** page, choose **Create a segment**.

2. On the **Create a segment** page, choose **Build a segment**.

3. For **Name**, enter a name for the segment.

4.Under **Segment group 1**, do the following:

* Next to **Include endpoints** that are in any of the following segments, choose the **Pinpoint_Sample_Import** segment that you created in the previous step.
* Under **Add filters to refine your segment**, from the menu, choose **Filter by channel**.

* Next to **Endpoints that match**, choose **all**.

* For **Channel**, choose **EMAIL**.

* Under **Add filters to refine your segment**, from the menu, choose **Filter by user**.

* In the **User** filter, use the menu to choose **Company**. Next, use the **Choose values** menu to choose the unique company name that you specified for your own contact record in step 2.1.
  
* Choose **Add an attribute or metric**.

* In the new filter, use the menu to choose **First Name**. Next, use the **Choose values** menu to choose your **first name**.

* Choose **Create segment**.

### Step 3: Create and Schedule a Campaign

A campaign is a messaging initiative that engages a specific audience segment. A campaign sends tailored messages on the days and times that you specify. You can use the console to create a campaign that sends messages through the email, push notification, or SMS channels.

In this section, you create an email campaign. You create a new campaign, choose your target segment, and create a responsive email message for the campaign. When you finish setting up the message, you choose the day and time when you want the message to be sent.

#### Step 3.1: Create the Campaign and Choose a Segment
When you create a segment, you first give the segment a name. Next, you choose the segment that the campaign applies to. In this tutorial, you choose the segment that you created in Step 2.3.

To create the campaign and choose segment

1. In a web browser, **download the sample file** from https://raw.githubusercontent.com/awsdocs/amazon-pinpoint-user-guide/master/examples/Pinpoint_Sample_Email.html. Save the file to your computer.

    ***Tip**
    You can quickly save this file to your computer by right-clicking the link, and then choosing Save Link As.*

2. **Open the file** that you just downloaded in a text editor, such as Notepad (Windows) or TextEdit (macOS). Press Ctrl+A (Windows) or Cmd+A (macOS) to select all of the text. Then, press Ctrl+C (Windows) or Cmd+C (macOS) to copy it.

3. In the **Amazon Pinpoint console**, in the navigation pane, choose **Campaigns**.

4. Choose **Create a campaign**.

5. Under **Campaign details**, for Campaign name, enter a **name** for the campaign.

6. For **Campaign type**, choose **Standard campaign**.

7. For Choose a **channel for this campaign**, choose **Email**.

8. Choose **Next**.

9. On the Choose a **segment page**, choose Use an existing segment. Then, for **Segment**, choose the **targeted segment that you created** in Step 2.3. Choose **Next**.

#### Step 3.2: Create the Campaign Message
After you specify a campaign name and choose a segment, you can create your message. This tutorial includes a link to an HTML file that you can use to create your message.

This sample file uses responsive HTML to create a message that renders properly on both computers and mobile devices. It uses inline CSS to provide compatibility with a wide variety of email clients. It also includes tags that are used to personalize the message with the recipient's name and other personal information.

To create the message

1. On the Create your message page, under **Message content**, choose **Create a new message**.

2. For **Subject**, enter a **subject line for the email**.

3. Under **Message**, erase the sample HTML code that's shown in the editor. Paste the HTML code that you copied in the first step in this section.

4. (Optional) Modify the content of the message to include a message that you want to send.

You can personalize the message for each recipient by including the name of an attribute inside two sets of curly braces. For example, the sample message includes the following text: {{User.UserAttributes.FirstName}}. This code represents the User.UserAttributes.FirstName attribute, which contains the recipient's first name. When you send the campaign, Amazon Pinpoint removes this attribute name and replaces it with the appropriate value for each recipient.

You can experiment with other attribute names. Refer to the column headers in the spreadsheet that you imported in Step 2.2 for complete list of attribute names that you can specify in your message.

***Tip**
You can use Design view to edit the content of the message without having to edit the HTML code. To use this view, choose Design from the view selector above the message editor, as shown in the following image.*


5.Choose **Next**.

##### Step 3.3: Schedule the Campaign
The last step in creating the campaign is to choose when to send it. In Amazon Pinpoint, you can set up your campaigns so that they're sent immediately after you launch them. You can also schedule them to be sent in the future—anywhere from 15 minutes from the current time, to six months into the future. Finally, you can schedule your messages to be sent on a recurring basis (that is, hourly, daily, weekly, or monthly). Recurring campaigns are a great way to send account or status updates where the appearance of the campaign message stays the same over time, but is populated with information that changes dynamically.

In this section, you schedule your campaign to be sent immediately after you launch it.

To schedule the campaign

1. On the **Choose when to send the campaign page**, choose At a **specific time**. Then, under **Choose** when the campaign should be sent, choose **Immediately**. Finally, choose **Next**.

2. On the **Review and launch page**, review all of the details of the **campaign**. When you're ready to send it, choose **Launch campaign**.

**Congratulations**—you've created your first campaign with Amazon Pinpoint! Because you're the only member of the segment that you created in Step 2.3, you should receive the message in your inbox within a few seconds.

### Step 4: View Campaign Analytics

At this point, you've created a segment that you're a member of. You've also created an email campaign and sent it to yourself. In this section, you look at the delivery and response metrics for the campaign.

#### Step 4.1: Interact with Your Campaign
Before you can view the delivery and response metrics for your campaign, you have to interact with the message that you sent yourself in Step 3.

**To interact with the email**

1. In your **email client**, open the message that you sent yourself in Step 3.

2. If your email client automatically hides images by default, choose the Download pictures (or equivalent) button to load the images in the message.

3. Choose one or more of the links that are contained in the message.

4. Wait for a few minutes, and then proceed to the next section.

#### Step 4.2: View Metrics for the Campaign
After you interact with the email that you sent from the campaign, you can view the metrics for the campaign.

**To view the campaign metrics**

1. Open the Amazon Pinpoint console at https://console.aws.amazon.com/pinpoint/.

2. On the All **projects page**, choose the **project that you used to send the campaign**.

3. In the **navigation pane**, under Analytics, choose **Campaigns**.

4. In the **Campaigns section**, choose the campaign that you created in Step 3.

5. (Optional) Use the date control to choose a date range for the reports on this page.

    On the metrics page for your campaign, you see the following information:

    * **Delivery count metrics** – This section provides information about the delivery of the messages that were sent from your campaign. It includes the following information:

      * **Messages sent** – The number of messages that were sent.

      * **Messages delivered** – The number of messages that were delivered to their recipients.

      * **Links clicked** – The number of times that links in the messages were clicked by recipients. If a single recipient clicks a link more than once, each click is represented in this section.

      * **Endpoint deliveries** – The average number of endpoints that the campaign was sent to, for each day in the chosen date range. The chart shows the number of endpoints that the campaign was delivered to, for each day in the chosen date range.

   * **Delivery rate metrics** – This section shows the overall delivery and response rates for the messages that were sent from your campaign. It includes the following information:

        * **Delivery rate** – The percentage of messages that were delivered to recipients, of the total number of endpoints that you targeted in the segment that you sent this campaign to.

       * **Email open rate** – The percentage of messages that were opened by recipients, of the total number of messages that were delivered.

       * **Bounce rate** – The percentage of messages that weren't delivered to recipients because they bounced. This value includes only hard bounces—that is, messages that bounced because of a permanent issue. For example, hard bounces could occur when the recipient's email address doesn't exist, or when the recipient permanently rejects email from your domain.

   * **Campaign runs** – This section shows information that's specific to each time the campaign ran. Because you can use Amazon Pinpoint to create recurring campaigns, this section can show information for several campaign runs. However, if you completed the procedures in this tutorial, this section contains information for only one campaign run because you ran the campaign only once. This section contains the following metrics, in addition to the metrics that are defined in the preceding sections:

     * **Endpoints targeted** – The number of endpoints that were targeted by the segment that was associated with the campaign run. This number includes endpoints that were part of the segment, but didn't receive the message.

     * **Total email opened** – The total number of times that messages sent from the campaign run were opened. For example, if a message was opened two times by one recipient, both of those opens are counted.

## Next Step

We hope that you use this tutorial as a starting point as you discover the additional capabilities of Amazon Pinpoint. For example:

* You can improve the delivery of your email campaigns by making sure that your campaigns align with industry best practices. For more information, see Tips and Best Practices.

* You can verify an entire domain, which allows you to send email from any address on that domain. For more information about verifying domains, see Verifying a Domain.

* You can obtain dedicated IP addresses for sending your email. Dedicated IP addresses are a great option for sending email in certain use cases. For more information, see Using Dedicated IP Addresses with Amazon Pinpoint.

* You can enable the Amazon Pinpoint Deliverability dashboard. The Deliverability dashboard helps you identify issues that could impact the delivery of your emails. For more information, see The Amazon Pinpoint Deliverability Dashboard.

* You can send messages through other channels, such as SMS or push. Before you can use these channels, you have to enable and configure them on the Settings page. For more information about using the Settings page to enable and configure channels, see Amazon Pinpoint Settings.

* You can send data about your campaigns outside of Amazon Pinpoint. For example, you can send delivery and response data for your campaigns to Amazon S3 for long-term storage. You can also send data to Amazon Redshift to perform custom analyses. For more information about sending your data outside of Amazon Pinpoint, see Event Stream Settings.

* You can integrate Amazon Pinpoint with your apps, or interact with Amazon Pinpoint programmatically, by using an AWS SDK. For more information, see the Amazon Pinpoint Developer Guide.</content><author><name>Girish Godage</name></author><category term="learning" /><summary type="html">What is Customer Engagement?</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/img/aws_cloud.png" /></entry><entry><title type="html">The service desk with bots using Amazon Lex and Amazon Connect (Part 2)</title><link href="http://localhost:4000/blog/create-service-desk-with-bots-using-amazon-lex-and-amazon-connect-part-2" rel="alternate" type="text/html" title="The service desk with bots using Amazon Lex and Amazon Connect (Part 2)" /><published>2020-02-02T11:41:00+00:00</published><updated>2020-02-02T11:41:00+00:00</updated><id>http://localhost:4000/blog/create-service-desk-with-bots-using-amazon-lex-and-amazon-connect-part-2</id><content type="html" xml:base="http://localhost:4000/blog/create-service-desk-with-bots-using-amazon-lex-and-amazon-connect-part-2">## The service desk with bots using Amazon Lex and Amazon Connect (Part 2)


Hopefully you've had the chance to follow along in parts 1  where we set up our Lex chatbot to take and validate input.

In this blog, we'll interface with our Active Directory environment to perform the password reset function. To do this, we need to create a Lambda function that will be used as the logic to fulfil the user's intent. The Lambda function will be packaged with the python LDAP library to modify the AD password attribute for the user. Below are the components that need to be configured.

## Active Directory Service Account

To begin, we need to start by creating a service account in Active Directory that has permissions to modify the password attribute for all users. Our Lambda function will then use this service account to perform password resets. To do this, create a service account in your Active Directory domain and perform the following to delegate the required permissions:

  1. Open Active Directory Users and Computers.
  2. Right click the OU or container that contains organisational users and click **Delegate Control**
  3. Click **Next** on the Welcome Wizard.
  4. Click **Add** and enter the service account that will be granted the reset password permission.
  5. Click **OK** once you've made your selection, followed by **Next**.
  6. Ensure that **Delegate the following common tasks** is enabled, and select **Reset user passwords and force password change at next logon**.
  7. Click **Next**, and **Finish**


![image info](/img/awscloud/19/delegatecontrol.png)

## KMS Key for the AD Service Account

Our Lambda function will need to use the credentials for the service account to perform password resets. We want to avoid storing credentials within our Lambda function, so we'll store the password as an encrypted Lambda variable and allow our Lambda function to decrypt it using Amazon's Key Management Service (KMS). To create the KMS encryption key, perform the following steps:

  1. In the Amazon Console, navigate to **IAM**
  2. Select **Encryption Keys**, then Create key
  3. Provide an Alias (e.g. resetpw) then select **Next**
  4. Select **Next Step** for all subsequent steps then **Finish** on step 5 to create the key.

## IAM Role for the Lambda Function

Because our Lambda function will need access to several AWS services such as SNS to notify the user of their new password and KMS to decrypt the service account password, we need to provide our function with an IAM role that has the relevant permissions. To do this, perform the following steps:


1. In the Amazon Console, navigate to **IAM**
2. Select **Policies** then **Create policy**
3. Switch to the **JSON** editor, then copy and paste the following policy, replacing the KMS resource with the KMS ARN created above
   ```
    {
    &quot;Version&quot;: &quot;2012-10-17&quot;,
    &quot;Statement&quot;: [
        {
        &quot;Effect&quot;: &quot;Allow&quot;,
        &quot;Action&quot;: &quot;sns:Publish&quot;,
        &quot;Resource&quot;: &quot;*&quot;
        },
        {
        &quot;Effect&quot;: &quot;Allow&quot;,
        &quot;Action&quot;: &quot;kms:Decrypt&quot;,
        &quot;Resource&quot;: &quot;&lt;KMS ARN&gt;&quot;
        }
    ]
    }
    ```
4. Provide a name for the policy (e.g. resetpw), then select **Create policy**
5. After the policy has been created, select **Roles**, then **Create Role**
6. For the AWS Service, select **Lambda**, then click **Next:Permissions**
7. Search for and select the policy you created in step 5, as well as the **AWSLambdaVPCAccessExecutionRole** and **AWSLambdaBasicExecutionRole** policies then click **Next: Review**
8. Provide a name for the role (e.g. resetpw) then click **Create role**

## Network Configuration for the Lambda Function

To access our Active Directory environment, the Lambda function will need to run within a VPC or a peered VPC that hosts an Active Directory domain controller. Additionally, we need the function to access the internet to be able to access the KMS and SNS services. Lambda functions provisioned in a VPC are assigned an Elastic network Interface with a private IP address from the subnet it's deployed in. Because the ENI doesn't have a public IP address, it cannot simply leverage an internet gateway attached to the VPC for internet connectivity and as a result, must have traffic routed through a NAT Gateway similar to the diagram below.

![image info](/img/awscloud/19/natgw.png)

## Password Reset Lambda Function

Now that we've performed all the preliminary steps, we can start building the Lambda function. Because the Lambda execution environment uses Amazon Linux, I prefer to build my Lambda deployment package on a local Amazon Linux docker container to ensure library compatibility. Alternatively, you could deploy a small Amazon Linux EC2 instance for this purpose, which can assist with troubleshooting if it's deployed in the same VPC as your AD environment.
Okay, so let's get started on building the lambda function. Log in to your Amazon Linux instance/container, and perform the following:


*  Create a project directory and install python-ldap dependencies, gcc, python-devel and openldap-devel

    *Mkdir ~/resetpw*

    *sudo yum install python-devel openldap-devel gcc*

*   Next, we're going to download the python-ldap library to the directory we created

    *Pip install python-ldap -t ~/resetpw*

* In the resetpw directory, create a file called reset_function.py and copy and paste the following script

```

import ldap
import os
import boto3
import random
import string

from base64 import b64decode

#Variables
url=os.environ['url']
domain_base_dn=os.environ['domain_base_dn']
user=os.environ['user']
ENCRYPTED=os.environ['pw']
pw=boto3.client('kms').decrypt(CiphertextBlob=b64decode(ENCRYPTED))['Plaintext']

def lambda_handler(event, context):
    #Set up LDAP connection
    ldap.set_option(ldap.OPT_X_TLS_REQUIRE_CERT, ldap.OPT_X_TLS_NEVER)
    con = ldap.initialize(url)
    con.set_option(ldap.OPT_REFERRALS, 0)
    con.bind_s(user,pw)
    slotUserID = event[&quot;currentIntent&quot;][&quot;slots&quot;][&quot;UserID&quot;]
    results = con.search_ext_s(domain_base_dn,ldap.SCOPE_SUBTREE,&quot;sAMAccountName=&quot; + slotUserID,attrlist=['monthStarted', 'birthDate', 'distinguishedName','telephoneNumber'])
    birthdate = results[0][1][&quot;birthDate&quot;][0]
    monthStarted = results[0][1][&quot;monthStarted&quot;][0]
    phoneNumber = results[0][1][&quot;telephoneNumber&quot;][0]
    slotBirthDate = event[&quot;currentIntent&quot;][&quot;slots&quot;][&quot;DOB&quot;]
    slotMonth = event[&quot;currentIntent&quot;][&quot;slots&quot;][&quot;MonthStarted&quot;]

    # Reset Password
    if((birthdate == slotBirthDate) and (monthStarted == slotMonth)):
        new_password = ''.join(random.choice(string.letters + string.digits + string.punctuation) for _ in range(10))
        unicode_pass = unicode('\&quot;' + new_password + '\&quot;', 'iso-8859-1')
        password_value = unicode_pass.encode('utf-16-le')
        add_pass = [(ldap.MOD_REPLACE, 'unicodePwd', [password_value])]
        con.modify_s(results[0][1][&quot;distinguishedName&quot;][0],add_pass)        
        sns = boto3.client('sns')
        sns.publish(PhoneNumber=phoneNumber, Message='Your new password is ' + new_password )
        endData = {
            &quot;dialogAction&quot;: {
                &quot;type&quot;: &quot;Close&quot;,
                &quot;fulfillmentState&quot;: &quot;Fulfilled&quot;,
                &quot;message&quot;: {
                    &quot;contentType&quot;: &quot;PlainText&quot;,
                    &quot;content&quot;: &quot;Your password has been reset and sent to your mobile.&quot;
                }
            }
        }
    else:
        endData = {
            &quot;dialogAction&quot;: {
                &quot;type&quot;: &quot;Close&quot;,
                &quot;fulfillmentState&quot;: &quot;Failed&quot;,
                &quot;message&quot;: {
                    &quot;contentType&quot;: &quot;PlainText&quot;,
                    &quot;content&quot;: &quot;The details you provided are incorrect. Goodbye&quot;
                }
            }
        }
    con.unbind()
    return endData
```

* Now, we need to create the Lambda deployment package. As the package size is correlated with the speed of Lambda function cold starts, we need to filter out anything that's not necessary to reduce the package size. The following zip's the script and LDAP library:

```

    Cd ~/resetpw
zip -r ~/resetpw.zip . -x &quot;setuptools*/*&quot; &quot;pkg_resources/*&quot; &quot;easy_install*&quot;*

```

* We need to deploy this package as a Lambda function. I've got AWSCLI installed in my Amazon Linux container, so I'm using the following CLI to create the Lambda function. Alternatively, you can download the zip file and manually create the Lambda function in the AWS console using the same parameters specified in the CLI below.

```


aws lambda create-function --function-name reset_function --region us-east-1 --zip-file fileb://root/resetpw.zip --role resetpw --handler reset_function.lambda_handler --runtime python2.7 --timeout 30 --vpc-config SubnetIds=subnet-a12b3cde,SecurityGroupIds=sg-0ab12c30 --memory-size 128*

```

*   For the script to run in your environment, a number of Lambda variables need to be set which will be used at runtime. In the AWS Console, navigate to Lambda then click on your newly created Lambda function. In the environment variables section, create the following variables:
  
    * **Url** – This is the LDAPS URL for your domain controller. Note that it must be LDAP over SSL.
    * **Domain_base_dn** – The base distinguished name used to search for the user
    * **User** – The service account that has permissions to change the user password
    * **Pw** – The password of the service account
  
* Finally, we need to encrypt the Pw variable in the Lambda console. Navigate to the Encryption configuration and select Enable helpers for encryption in transit. Select your KMS key for both encryption in transit and at reset, then select the Encrypt button next to the pw variable. This will encrypt and mask the value.

![image info](/img/awscloud/19/envvar.png)

* Hit Save in the top right-hand corner to save the environment variables


That's it! The Lambda function is now configured. A summary of the Lambda function's logic is as follows:

1. Collect the Lambda environment variables and decrypt the service account password
2. Perform a secure AD bind but don't verify the certificate (I'm using a Self-Signed Cert in my lab)
3. Collect the user's birthday, start month, telephone number, and DN from AD
4. Check the user's verification input
5. If the input is correct, reset the password and send it to the user's telephone number, otherwise exit the function.


## Update and test Lex bot fulfillment

The final step is to add the newly created Lambda function to the Lex bot so it's invoked after input is validated. To do this, perform the following

1. In the Amazon Console, navigate to Amazon Lex and select your bot
2. Select Fulfillment for your password reset intent, then select AWS Lambda function
3. Choose the recently created Lambda function from the drop down box, then select Save Intent

That should be it! You'll need to build your bot again before testing…

![image info](/img/awscloud/19/fulfilled.png)

My phone buzzes with a new SMS…

![image info](/img/awscloud/19/passmsg.png)  

Success! A few final things worth noting:

All Lambda execution logs will be written to CloudWatch logs, so this is the best place to begin troubleshooting if required
Modification to the AD password attribute is not possible without a secure bind and will result in an error.
The interrogated fields (month started and date of birth) are custom AD attributes in my lab.
Password complexity in my lab is relaxed. If you're using the default password complexity in AD, the randomly generated password in the lambda function may not meet complexity requirements every time.


While we've successfully tested this functionality in the AWS console, we want to provide our users with the ability to call and talk with the bot over the phone. Here, we'll wire up Amazon Connect with our bot to provide this capability.

## What is Amazon Connect
Amazon Connect is a Cloud based contact service center that can be set up in minutes to take phone calls and route them to the correct service center agents. Additionally, Connect is able to integrate with Amazon Lex to create a self-service experience, providing a cost effective method for resolving customer queries without having to wait in queue for a human agent. In our case, Lex will be integrated with Amazon Connect to field password reset requests.

### Provisioning Amazon Connect

The following steps provision the base Amazon Connect tenant:

1. Begin by heading to the AWS Console, then navigate to **Amazon Connect** and select **Add an instance**.
2. Specify a sub-domain for the access URL which will be used to log into Amazon Connect. Select **Next step**.
3. For now, skip creating an administrator by selecting **Skip this**, then select Next step.
4. For Telephony Options ensure **Incoming Calls** is selected and **Outbound Calls** is unselected, then click **Next step**.
5. Accept the default data storage configuration and select **Next step**.
6. Finally, review the information provided and select **Create instance.**

That's all that's required to provision the Amazon Connect service. Pretty simple stuff. It takes a few minutes to provision, then you'll be ready to begin configuring your Amazon Connect tenant.

### Configuring Amazon Connect

Next, we need to claim a phone number to be used with our service:

1. Once Amazon Connect has been provisioned, click **Get started** to log into your Amazon Connect instance.
2. On the Welcome to Amazon Connect page, select **Let's Go.**
3. To claim a phone number, select your preferred country code and type then select **Next**. You may find that there are no available numbers for your country of choice (like the screenshot below). If that's the case and it's necessary that you have a local number, you can raise a support case with Amazon. For testing purposes, I'm happy to provision a US number and use Google Hangouts to dial the number for free.
   
   ![image info](/img/awscloud/19/claimphonenumber.png)  

4. When prompted to make a call, choose Skip for now.   

You should now be at the Amazon Connect dashboard where there are several options, but before we can continue, we first need to add the Lex bot to Amazon Connect to allow it to be used within a contact flow.

### Adding Lex to Amazon Connect
1. Switch back to the AWS Console and navigate to **Amazon Connect**.
2. Select the Amazon Connect instance alias created in the previous step.
3. On the left-hand side, select **Contact Flows**.
4. Under the Amazon Lex section, click **Add Lex Bot**, and select the user administration bot we created.
5. Select Save Lex Bots.
   
Now that our bot has been added to Amazon Connect, we should be able to create an appropriate Contact Flow that leverages our bot.

### Creating the Contact Flow

1. Switch back to the Amazon Connect dashboard then navigate to **Contact Flows** under routing on the left sidebar.
2. Select **Create contact flow** and enter a name (e.g. User administration) for the contact flow.
3. Expand the **Interact** menu item then click and drag **Get customer input** to the grid.
4. Click the **Get customer input** item, and set the following properties:
   * Enable **Text to speech** then add a greeting text (e.g. Welcome to the cloud call center. What would you like assistance with?).
   * Ensure that **Interpret** as is set to **Text**
   * Choose the **Amazon Lex** option, then add the Lex Bot name (e.g. UserAdministration) and set the alias to **$LATEST** to ensure it uses the latest build of the bot.
5. Under Intents, select **Add a parameter** then enter the password reset intent for the Lex Bot (e.g. ResetPW)
6. Select **Save** to save the configuration.
   
    ![image info](/img/awscloud/19/customerinput.png) 

    It's worth noting that if you wanted to send the user's mobile number through to your Lex bot for verification purposes, this can be done by sending a session attribute as shown below. The phone number will be passed to the Lambda function in the sessionAttributes object.

    ![image info](/img/awscloud/19/sessionattribute.png) 

7. On the left sidebar, expand Terminate/Transfer then drag and drop Disconnect/Hang up onto the grid.

8. Connect the Start entry point to the Get Customer Input box and connect all the branches of the Get Customer Input Box to the Disconnect/Hang up box as shown below.

    ![image info](/img/awscloud/19/contactflow.png) 

    We could have added more complex flows to deal with unrecognised intents or handle additional requests that our Lex bot isn't configured for (both of which would be forwarded to a human agent), however this is outside the scope of this blog post.

9. In the top right-hand corner above the grid, select the down arrow, then **Save &amp; Publish**.

## Setting the Contact Flow
Now that we have a contact flow created, we need to attach it to the phone number we provisioned earlier.

1. On the left sidebar in the Amazon Connect console, select **Phone Numbers** under the **Routing** menu then select the phone number listed.
2. Under the **Contact Flow/IVR** dropdown menu, select the Contact flow you created, then select **Save**.
   
## Testing the Contact Flow

Now that we've associated the contact flow with the phone number, you're ready for testing! Remember, if you've provisioned a number in the US (and you're overseas), you can dial for free using Google hangouts.

That's it! You now have a fully functioning chatbot that can be called and spoken to. From here, you can add more intents to build a bot that can handle several simple user administration tasks.

A few things worth noting:

* You may notice that Lex repeats the user ID as a number, rather than individual digits. Unfortunately, Amazon Connect doesn't support SSML content from Lex at this time however it's in the product roadmap.
* You can view missed utterances on the **Monitoring tab** on your Lex bot and potentially add them to existing intents. This is a great way to monitor and expand on the capability of your bot.




{% include blogslide.html %}</content><author><name>Girish Godage</name></author><category term="learning" /><summary type="html">The service desk with bots using Amazon Lex and Amazon Connect (Part 2)</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/img/aws_cloud.png" /></entry><entry><title type="html">The service desk with bots using Amazon Lex and Amazon Connect (Part 1)</title><link href="http://localhost:4000/blog/create-service-desk-with-bots-using-amazon-lex-and-amazon-connect-part-1" rel="alternate" type="text/html" title="The service desk with bots using Amazon Lex and Amazon Connect (Part 1)" /><published>2020-02-02T11:41:00+00:00</published><updated>2020-02-02T11:41:00+00:00</updated><id>http://localhost:4000/blog/create-service-desk-with-bots-using-amazon-lex-and-amazon-connect-part-1</id><content type="html" xml:base="http://localhost:4000/blog/create-service-desk-with-bots-using-amazon-lex-and-amazon-connect-part-1">## The service desk with bots using Amazon Lex and Amazon Connect (Part 1)

What! Is this guy for real? Does he really think he can replace the front line of IT with pre-canned conversations?&quot; I must admit, it's a bold statement. The IT Service Desk has been around for years and has been the foot in the door for most of us in the IT industry. It's the face of IT operations and plays an important role in ensuring an organisation's staff can perform to the best of their abilities. But what if we could take some of the repetitive tasks the service desk performs and automate them? Not only would we be saving on head count costs, we would be able to ensure correct policies and procedures are followed to uphold security and compliance. The aim of this blog is to provide a working example of the automation of one service desk scenario to show how easy and accessible the technology is, and how it can be applied to various use cases.

To make it easier to follow along, I've broken this blog up into a number of parts. Part 1 will focus on the high-level architecture for the scenario and begin creating the Lex chatbot.

![image info](/img/awscloud/19/selfservicedora.png)

## Scenario
Arguably, the most common service desk request is the password reset. While this is a pretty simple issue for the service desk to resolve, many service desk staff seem to skip over, or not realise the importance of user verification. Both the simple resolution and the strict verification requirement make this a prime scenario to automate.

## Architecture
So what does the architecture look like? The diagram below dictates the expected process flow. Let's step through each item in the diagram.

![image info](/img/awscloud/19/amazon-connect-page-1-1.png)

### Amazon Connect
The process begins when the user calls the service desk and requests to have their password reset. In our architecture, the service desk uses Amazon Connect which is a cloud based customer contact centre service, allowing you to create contact flows, manage agents, and track performance metrics. We're also able to plug in an Amazon Lex chatbot to handle user requests and offload the call to a human if the chatbot is unable to understand the user's intent.

### Amazon Lex
After the user has stated their request to change their password, we need to begin the user verification process. Their intent is recognised by our Amazon Lex chatbot, which initiates the dialog for the user verification process to ensure they are who they really say they are.

### AWS Lambda
After the user has provided verification information, AWS Lambda, which is a compute on demand service, is used to validate the user's input and verify it against internal records. To do this, Lambda interrogates Active Directory to validate the user.

### Amazon SNS
Once the user has been validated, their password is reset to a random string in Active Directory and the password is messaged to the user's phone using Amazon's Simple Notification Service. This completes the interaction.

## Building our Chatbot
Before we get into the details, it's worth mentioning that the aim of this blog is to convey the technology capability. There's many ways of enhancing the solution or improving validation of user input that I've skipped over, so while this isn't a finished production ready product, it's certainly a good foundation to begin building an automated call centre.

To begin, let's start with building our Chatbot in **Amazon Lex**. In the Amazon Console, navigate to Amazon Lex. You'll notice it's only available in Ireland and US East. As Amazon Connect and my Active Directory environment is also in US East, that's the region I've chosen.

Go ahead and select **Create Bot**, then choose to create your own **Custom Bot**. I've named mine &quot;**UserAdministration**&quot;. Choose an Output voice and set the session timeout to 5 minutes. An IAM Role will automatically be created on your behalf to allow your bot to use Amazon Polly for speech. For COPPA, select **No**, then select **Create**.

![image info](/img/awscloud/19/custombot.png)

Once the bot has been created, we need to identify the user action expected to be performed, which is known as an intent. A bot can have multiple intents, but for our scenario, we're only creating one, which is the password reset intent. Go ahead and select **Create Intent**, then in the Add Intent window, select **Create new intent**. My intent name is **&quot;ResetPW&quot;**. Select **Add**, which should add the intent to your bot. We now need to specify some expected sample utterances, which are phrases the user can use to trigger the Reset Password intent. There's quite a few that could be listed here, but I'm going to limit mine to the following:

* **I forgot my password**
* **I need to reset my password**
* **Can you please reset my password**

![image info](/img/awscloud/19/utterances.png)

The next section is the configuration for the Lambda validation function. Let's skip past this for the time being and move onto the slots. Slots are used to collect information from the user. In our case, we need to collect verification information to ensure the user is who they say they are. The verification information collected is going to vary between environments. I'm looking to collect the following to verify against Active Directory:

* **User ID** – In my case, this is a 6-digit employee number that is also the sAMAccountName in Active Directory
* **User's birthday** – This is a custom attribute in my Active Directory
* **Month started** – This is a custom attribute in my Active Directory

In addition to this, it's also worth collecting and verifying the user's mobile number. This can be done by passing the caller ID information from Amazon Connect, however we'll skip this, as the bulk of our testing will be text chat and we need to ensure we have a consistent experience.

To define a slot, we need to specify three items:

* **Name of the slot** – Think of this as the variable name.
* **Slot type** – The data type expected. This is used to train the machine learning model to recognise the value for the slot.
* **Prompt** – How the user is prompted to provide the value sought.

Many slot types are provided by Amazon, two of which has been used in this scenario. For &quot;MonthStarted&quot;, I've decided to create my own custom slot type, as the in-built &quot;AMAZON.Month&quot; slot type wasn't strictly enforcing recognisable months. To create your own slot type, press the **plus** symbol on the left-hand side of the page next to **Slot Types**, then provide a name and description for your slot type. Select to **Restrict to Slot values and Synonyms**, then enter each month and its abbreviation. Once completed, click Add slot to intent.

![image info](/img/awscloud/19/monthsofyear.png)

Once the custom slot type has been configured, it's time to set up the slots for the intent. The screenshot below shows the slots that have been configured and the expected order to prompt the user.

![image info](/img/awscloud/19/slots.png)

Last step (for this blog post), is to have the bot verify the information collected is correct. Tick the Confirmation Prompt box and in the Confirm text box provided, enter the following:

Just to confirm, your user ID is {UserID}, your Date of Birth is {DOB} and the month you started is {MonthStarted}. Is this correct?

For the Cancel text box, enter the following:
Sorry about that. Please call back and try again.

![image info](/img/awscloud/19/confirmation.png)

Be sure to leave the **fulfillment** to **Return parameters to client** and hit **Save Intent**.

Great! We've built the bare basics of our bot. It doesn't really do much yet, but let's take it for a spin anyway and get a feel for what to expect. In the top right-hand corner, there's a **build** button. Go ahead and click the button.

This takes some time, as building a bot triggers machine learning and creates the models for your bot. Once completed, the bot should be available to text or voice chat on the right side of the page. As you move through the prompts, you can see at the bottom the slots getting populated with the expected format. i.e. 14th April 1983 is converted to 1983-04-14.

![image info](/img/awscloud/19/chatlog.png)

So at the moment, our bot doesn't do much but collect the information we need. Admittedly, the conversation is a bit robotic as well. In the next few blogs, we'll give the bot a bit more of a personality, we'll do some input validation, and we'll begin to integrate with Active Directory. Once we've got our bot working as expected, we'll bolt on Amazon Connect to allow users to dial in and converse with our new bot.

## Creating the Lambda initialisation and validation function

As data validation requires compute, we'll need to start by creating an AWS Lambda function. Head over to the AWS console, then navigate to the AWS Lambda page. Once you're there, select Create Function and choose to Author from Scratch then specify the following:

* **Name**: ResetPWCheck
* **Runtime**: Python 2.7 (it's really a matter of preference)
* **Role**: I use an existing Out of the Box role, &quot;Lambda_basic_execution&quot;, as I only need access to CloudWatch logs for debugging.

![image info](/img/awscloud/19/createfunction.png)

Once you've populated all the fields, go ahead and select Create Function. The script we'll be using is provided (further down) in this blog, however before we go through the script in detail, there are two items worth mentioning.

### Input Events and Response Formats
It's well worth familiarising yourself with the page on Lambda Function Input Event and Response Formats in the Lex Developer guide. Every time input is provided to Lex, it invokes the Lambda initalisation and validation function.
***For example**, when I tell my chatbot &quot;I need to reset my password&quot;, the lambda function is invoked and the following event is passed:*


```python
{
    &quot;currentIntent&quot;: {
        &quot;slots&quot;: {
            &quot;DOB&quot;: None,
            &quot;MonthStarted&quot;: None,
            &quot;UserID&quot;: None
        },
        &quot;confirmationStatus&quot;: &quot;None&quot;,
        &quot;name&quot;: &quot;ResetPW&quot;,
        &quot;slotDetails&quot;: {
            &quot;DOB&quot;: {
                &quot;originalValue&quot;: None,
                &quot;resolutions&quot;: []
            },
            &quot;MonthStarted&quot;: {
                &quot;originalValue&quot;: None,
                &quot;resolutions&quot;: []
            },
            &quot;UserID&quot;: {
                &quot;originalValue&quot;: None,
                &quot;resolutions&quot;: []
            }
        }
    },
    &quot;userId&quot;: &quot;sd0xe8xzbgf1kyvtbnzsuymxky2a5bnq&quot;,
    &quot;bot&quot;: {
        &quot;alias&quot;: &quot;$LATEST&quot;,
        &quot;version&quot;: &quot;$LATEST&quot;,
        &quot;name&quot;: &quot;UserAdministration&quot;
    },
    &quot;inputTranscript&quot;: &quot;I need to reset my password&quot;,
    &quot;requestAttributes&quot;: None,
    &quot;invocationSource&quot;: &quot;DialogCodeHook&quot;,
    &quot;outputDialogMode&quot;: &quot;Text&quot;,
    &quot;messageVersion&quot;: &quot;1.0&quot;,
    &quot;sessionAttributes&quot;: {}
}
```
Amazon Lex expects a response from the Lambda function in JSON format that provides it with the next dialog action.

### Persisting Variables with Session Attributes

There are many ways to determine within your Lambda function where you're up to in your chat dialog, however my preferred method is to pass state information within the SessionAttributes object of the input event and response as a key/value pair. The SessionAttributes can persist between invocations of the Lambda function (every time input is provided to the chatbot), however you must remember to collect and pass the attributes between input and responses to ensure it persists.

### Input Validation Code
With that out of the way, let's begin looking at the code. The below script is what I've used which you can simply copy and paste, assuming you're using the same slot and intent names in your Lex bot that were used.

```python
def elicit_slot(messageType, messageContent, slots, slotToElicit, completed):
    response = {
        &quot;sessionAttributes&quot;: {
            &quot;Completed&quot;: completed
        },
        &quot;dialogAction&quot;: {
            &quot;type&quot;: &quot;ElicitSlot&quot;,
            &quot;message&quot;: {
                &quot;contentType&quot;: messageType,
                &quot;content&quot;: messageContent
            },
            &quot;intentName&quot;: &quot;ResetPW&quot;,
            &quot;slots&quot;: slots,
            &quot;slotToElicit&quot;: slotToElicit
        }
    }
    return response


def delegate_slot(slots, completed):
    response = {
        &quot;sessionAttributes&quot;: {
            &quot;Completed&quot;: completed
        },
        &quot;dialogAction&quot;: {
            &quot;type&quot;: &quot;Delegate&quot;,
            &quot;slots&quot;: slots
        }
    }
    return response


def chkUserId(slots, sessionAttributes):
    if((slots['UserID'] is None) or ((slots[&quot;UserID&quot;] is not None) and (len(slots[&quot;UserID&quot;]) != 6))):
        messageContent = &quot;That format seems incorrect. Let's try again. What is your user id?&quot;
        return elicit_slot(&quot;PlainText&quot;, messageContent, slots, &quot;UserID&quot;, &quot;None&quot;)
    else:
        return delegate_slot(slots, &quot;userid&quot;)


def chkDob(slots, sessionAttributes):
    if(slots[&quot;DOB&quot;] is None):
        messageContent = &quot;That format seems incorrect. Let's try again. What is your Date of Birth?&quot;
        return elicit_slot(&quot;PlainText&quot;, messageContent, slots, &quot;DOB&quot;, &quot;userid&quot;)
    else:
        return delegate_slot(slots, &quot;dob&quot;)


def chkMonth(slots, sessionAttributes, confirmation, outputDialogMode):
    if(slots[&quot;MonthStarted&quot;] is None):
        messageContent = &quot;That month seems incorrect. Let's try again. What month did you start?&quot;
        return elicit_slot(&quot;PlainText&quot;, messageContent, slots, &quot;MonthStarted&quot;, &quot;dob&quot;)
    else:
        return confirmIntent(slots, sessionAttributes, confirmation, outputDialogMode)


def confirmIntent(slots, sessionAttributes, confirmation, outputDialogMode):
    if(confirmation == &quot;None&quot;):
        endData = {
            &quot;dialogAction&quot;: {
                &quot;type&quot;: &quot;ConfirmIntent&quot;,
                &quot;message&quot;: {
                },
                &quot;intentName&quot;: &quot;ResetPW&quot;,
                &quot;slots&quot;: slots
            }
        }
        if(outputDialogMode == &quot;Voice&quot;):
            endData[&quot;dialogAction&quot;][&quot;message&quot;][&quot;contentType&quot;] = &quot;SSML&quot;
            endData[&quot;dialogAction&quot;][&quot;message&quot;][&quot;content&quot;] = &quot;&lt;speak&gt;Just to confirm, your user id is &lt;say-as interpret-as=\&quot;digits\&quot;&gt;&quot; + slots[&quot;UserID&quot;] + \
                &quot;&lt;/say-as&gt;, your date of birth is &quot; + \
                slots[&quot;DOB&quot;] + &quot; and the month of your start date is &quot; + \
                slots[&quot;MonthStarted&quot;] + &quot;. Is that correct?&lt;/speak&gt;&quot;
        else:
            endData[&quot;dialogAction&quot;][&quot;message&quot;][&quot;contentType&quot;] = &quot;PlainText&quot;
            endData[&quot;dialogAction&quot;][&quot;message&quot;][&quot;content&quot;] = &quot;Just to confirm, your user id is &quot; + slots[&quot;UserID&quot;] + \
                &quot;, your date of birth is &quot; + \
                slots[&quot;DOB&quot;] + &quot; and the month of your start date is &quot; + \
                slots[&quot;MonthStarted&quot;] + &quot;. Is that correct?&quot;
    elif((confirmation == &quot;Denied&quot;)):
        endData = {
            &quot;dialogAction&quot;: {
                &quot;type&quot;: &quot;Close&quot;,
                &quot;fulfillmentState&quot;: &quot;Failed&quot;,
                &quot;message&quot;: {
                    &quot;contentType&quot;: &quot;PlainText&quot;,
                    &quot;content&quot;: &quot;Please call back and try again.&quot;
                }
            }
        }
    else:
        endData = delegate_slot(slots, &quot;confirmed&quot;)
    return endData


def lambda_handler(event, context):
    slots = event['currentIntent']['slots']
    sessionAttributes = event['sessionAttributes']
    confirmation = event['currentIntent'][&quot;confirmationStatus&quot;]
    outputDialogMode = event[&quot;outputDialogMode&quot;]

    if(sessionAttributes is None or &quot;Completed&quot; not in sessionAttributes):
        messageContent = &quot;Sure, i can help with that! But first, i need to verify your identity. What is your user id?&quot;
        return elicit_slot(&quot;PlainText&quot;, messageContent, slots, &quot;UserID&quot;, &quot;None&quot;)
    
    if(sessionAttributes[&quot;Completed&quot;] == &quot;None&quot;):
        return chkUserId(slots, sessionAttributes)

    if(sessionAttributes[&quot;Completed&quot;] == &quot;userid&quot;):
        return chkDob(slots, sessionAttributes)

    if(sessionAttributes[&quot;Completed&quot;] == &quot;dob&quot;):
        return chkMonth(slots, sessionAttributes, confirmation, outputDialogMode)
```

Let's break it down.

When the lambda function is first invoked, we check to see if any state is set in the sessionAttributes. If not, we can assume this is the first time the lambda function is invoked and as a result, provide a welcoming response while requesting the User's ID. To ensure the user isn't welcomed again, we set a session state so the Lambda function knows to move to User ID validation when next invoked. This is done by setting the &quot;Completed&quot; : &quot;None&quot; key/value pair in the response SessionAttributes.

```python
if(sessionAttributes is None or &quot;Completed&quot; not in sessionAttributes):
        messageContent = &quot;Sure, i can help with that! But first, i need to verify your identity. What is your user id?&quot;
        return elicit_slot(&quot;PlainText&quot;, messageContent, slots, &quot;UserID&quot;, &quot;None&quot;)
```

Next, we check the User ID. You'll notice the chkUserId function checks for two things; That the slot is populated, and if it is, the length of the field. Because the slot type is AMAZON.Number, any non-numeric characters that are entered will be rejected by the slot. If this occurs, the slot will be left empty, hence this is something we're looking out for. We also want to ensure the User ID is 6 digits, otherwise it is considered invalid. If the input is correct, we set the session state key/value pair to indicate User ID validation is complete then allow the dialog to continue, otherwise we request the user to re-enter their User ID.


```python
def chkUserId(slots, sessionAttributes):
    if((slots['UserID'] is None) or ((slots[&quot;UserID&quot;] is not None) and (len(slots[&quot;UserID&quot;]) != 6))):
        messageContent = &quot;That format seems incorrect. Let's try again. What is your user id?&quot;
        return elicit_slot(&quot;PlainText&quot;, messageContent, slots, &quot;UserID&quot;, &quot;None&quot;)
    else:
        return delegate_slot(slots, &quot;userid&quot;)
```

Next, we check the Date of Birth. Because the slot type is strict regarding input, we don't do much validation here. An utterance for this slot type generally maps to a complete date: YYYY-MM-DD. For validation purpose, we're just looking for an empty slot. Like the User ID check, we set the session state and allow the dialog to continue if all looks good.

```python
def chkDob(slots, sessionAttributes):
    if(slots[&quot;DOB&quot;] is None):
        messageContent = &quot;That format seems incorrect. Let's try again. What is your Date of Birth?&quot;
        return elicit_slot(&quot;PlainText&quot;, messageContent, slots, &quot;DOB&quot;, &quot;userid&quot;)
    else:
        return delegate_slot(slots, &quot;dob&quot;)
```

Finally, we check the last slot which is the Month Started. Assuming the input for the month started is correct, we then confirm the intent by reading all the slot values back to the user and asking if it's correct. You'll notice here that there's a bit of logic to determine if the user is using voice or text to interact with Lex. If voice is used, we use Speech Synthesis Markup Language (SSML) to ensure the UserID value is read as digits, rather than as the full number.
If the user is happy with the slot values, the validation completes and Lex then moves to the next Lambda function to fulfil the intent (next blog). If the user isn't happy with the slot values, the lambda function exits telling the user to call back and try again.

```python
def confirmIntent(slots, sessionAttributes, confirmation, outputDialogMode):
    if(confirmation == &quot;None&quot;):
        endData = {
            &quot;dialogAction&quot;: {
                &quot;type&quot;: &quot;ConfirmIntent&quot;,
                &quot;message&quot;: {
                },
                &quot;intentName&quot;: &quot;ResetPW&quot;,
                &quot;slots&quot;: slots
            }
        }
        if(outputDialogMode == &quot;Voice&quot;):
            endData[&quot;dialogAction&quot;][&quot;message&quot;][&quot;contentType&quot;] = &quot;SSML&quot;
            endData[&quot;dialogAction&quot;][&quot;message&quot;][&quot;content&quot;] = &quot;&lt;speak&gt;Just to confirm, your user id is &lt;say-as interpret-as=\&quot;digits\&quot;&gt;&quot; + slots[&quot;UserID&quot;] + \
                &quot;&lt;/say-as&gt;, your date of birth is &quot; + \
                slots[&quot;DOB&quot;] + &quot; and the month of your start date is &quot; + \
                slots[&quot;MonthStarted&quot;] + &quot;. Is that correct?&lt;/speak&gt;&quot;
        else:
            endData[&quot;dialogAction&quot;][&quot;message&quot;][&quot;contentType&quot;] = &quot;PlainText&quot;
            endData[&quot;dialogAction&quot;][&quot;message&quot;][&quot;content&quot;] = &quot;Just to confirm, your user id is &quot; + slots[&quot;UserID&quot;] + \
                &quot;, your date of birth is &quot; + \
                slots[&quot;DOB&quot;] + &quot; and the month of your start date is &quot; + \
                slots[&quot;MonthStarted&quot;] + &quot;. Is that correct?&quot;
    elif((confirmation == &quot;Denied&quot;)):
        endData = {
            &quot;dialogAction&quot;: {
                &quot;type&quot;: &quot;Close&quot;,
                &quot;fulfillmentState&quot;: &quot;Failed&quot;,
                &quot;message&quot;: {
                    &quot;contentType&quot;: &quot;PlainText&quot;,
                    &quot;content&quot;: &quot;Please call back and try again.&quot;
                }
            }
        }
    else:
        endData = delegate_slot(slots, &quot;confirmed&quot;)
    return endData
```

Okay, now that our Lambda function is finished, we need to enable it as a code hook for initialisation and validation. Head over to your Lex bot, select the &quot;ResetPW&quot; intent, then tick the box under **Lambda initialisation and validation** and select your Lambda function. A prompt will be given to provide permissions to allow your Lex bot to invoke the lambda function. Select **OK**.


![image info](/img/awscloud/19/lambdaval.png)

Let's hit Build on the chatbot, and test it out.

![image info](/img/awscloud/19/chatlog2.png)

So, we've managed to make the conversation a bit more human like and we can now detect invalid input. If you use the microphone to chat with your bot, you'll notice the User ID value is read as digits. That's it for this blog. Next blog, we'll integrate Active Directory and actually get a password reset and sent via SNS to a mobile phone.

{% include blogslide.html %}</content><author><name>Girish Godage</name></author><category term="learning" /><summary type="html">The service desk with bots using Amazon Lex and Amazon Connect (Part 1)</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/img/aws_cloud.png" /></entry><entry><title type="html">Key Benefits of Angular &amp;amp; Use Cases</title><link href="http://localhost:4000/blog/key-benefits-angular-use-cases" rel="alternate" type="text/html" title="Key Benefits of Angular &amp; Use Cases" /><published>2020-02-02T10:41:00+00:00</published><updated>2020-02-02T10:41:00+00:00</updated><id>http://localhost:4000/blog/key-benefits-angular-use-cases</id><content type="html" xml:base="http://localhost:4000/blog/key-benefits-angular-use-cases">## Key Benefits of Angular &amp; Use Cases

 Have you ever struggled to optimize server communication for your application? Or found it difficult to handle auto-synchronization in an application? You've come to the right place. Angular, as one of the top frontend framework, handles this proactively. Let's discuss some of the key benefits and widely applied use cases of Angular.


![image info](/img/webdevelopment/6/angularjs-benefits-final-BANNER-MARCH.png)

Angular has come a long way since its inception. The Angular team at Google keeps the framework upgraded with the constant evolution of browsers and their ability to support diverse functionalities over time. And today, it is one of the most preferred choices for **developing feature-rich web applications.**

Its user interface and functionality have been acknowledged by several companies and developer communities from time and again. Thanks to Angular business benefits, **over 72,924 websites** in the US alone use the framework for website development. 

Developers understand the critical challenges and shortcomings in developing a Single Page Application (SPA). While the initial Angular development benefits were more inclined towards tackling that challenge, it gradually evolved to fix several concerns, including mobile optimization, progressive web app development, and more.

We work on various small and large scale projects. While some use Angular, a lot of them use React, a bunch of them are exploring Vue.js, and a few are still using jQuery. This post is directed towards people who are considering Angular &amp; TypeScript as their choice for frontend engineering. Let's dig deeper into the Agular advantages and disadvantages to help you evaluate your decision better.

## Benefits of Angular for Web Development

*  Automatic Synchronization with Two-Way Data Binding
*  Optimized Server Communication
*  POJO Model to Eliminate External Dependencies
*  Testing in Angular – An Integral Part
*  Angular Material – A Comprehensive Modern UI
*  Angular &amp; Single Page Applications
*  Code Consistency and Robust Ecosystem
*  Model-View-View-Model (MMVC) Architecture  to Unify Independent Structures
*  Ivy-Renderer – Next-Generation Renderer for Enhanced Performance
*  Active Community &amp; Easy-to Access Information
*  Improve Productivity with Code Reusability
*  Design Development Workflow
*  Easy to Use
*  When to Use Angular
*  When not to Use Angular

### Automatic Synchronization with Two-Way Data Binding
With most frameworks offering one-way data binding, this feature gives Angular an edge over the others. So what's the deal with two-way data binding? 

It synchronizes the data between Model and View. As a result, when data is changed or modified, these two components automatically get updated. What's more, this happens in real-time, preventing developers from putting extra efforts into manual modifications.

Unlike other web frameworks, which implement two-way data binding by using fragile event listeners and handlers, Angular makes this process an integral function of its architecture. Consequently, there's little need for sequential callbacks to handle modified data or developer interventions. 

### Optimized Server Communication
Caching becomes trouble-free with Angular!

Angular reduces the extra burden of CPUs by creating static files. Besides, the response time to API calls is tremendously quick in the framework. 

 Referring to an Angular application, we can see it renders the pages in the document object model for the user actions. The framework reduces its efforts just by generating the static app pages, unlike other tech stacks. It gives an overall quick user experience and does not let the users wait until the app becomes completely interactive. 

### POJO Model to Eliminate External Dependencies
Angular employs the Plain Old Javascript Objects (POJO) Model to make the code structure handy and independent. This way, we save ourselves from adding complex functions or methods in the programme. Additionally, it eliminates dependencies on external frameworks or plugins.

 The model enables us to keep our codes clean, making the framework goal-oriented since POJO requires less coding, the applications built with Angular load quickly, and offer excellent user accessibility.

### Testing in Angular – An Integral Part
Testing is an integral part of the Angular framework. All JavaScript code in Angular is required to go through a series of tests. The convenience offered in testing allows us to develop the project from scratch and seamlessly test the components. 

 Thanks to the dependency injection in Angular. It manages all the scopes and controllers. Because of this dependency, the unit test functionality in Angular can force dependency injection to perform testing by injecting mock/dummy data into the controller. This process is followed by the assessment of the output and its behavior. What's more interesting is that Angular also has mock HTTP providers to push fake responses from servers into controllers.

### Angular Material – A Comprehensive Modern UI
The Angular Materials follows the Material Design guidelines by Google. It provides in-built components such as navigation elements, layout, button, indicators, and data tables.

Moreover, Angular Material is not just about creating an eye-pleasant user interface, but it develops applications like Google. For instance, apps like Google drive, Android OS, Gmail, and many have become daily needs of users. 

### Angular &amp; Single Page Applications
There are two aspects to the smooth functioning of any single page application. One is that all the mandatory codes of JavaScript, CSS, and HTML are retrieved at one single instance when the page loads. The second is that they are retrieved as and when required depending on user behavior and action.

![image info](/img/webdevelopment/6/microsoftSPA.png)

There is no page reload happening at any point in time while users are browsing the app, which in turn involves dynamic communication with servers under the hood. However, all these complexities are easily handled by Angular. If your product idea revolves around the development of a radical single-page application, preferring Angular for its development would be the best decision you would make.

### Code Consistency and Robust Ecosystem
For any strong development environment to be successful, code consistency is the key. Thanks to the Angular CLI and documentation style guide, they both drive consistency at the pioneer level. 

Angular CLI tool lets the developer create initial projects, run the tests, and add different features in the same project while keeping the entire team on the same page. Often, I found developers to be thankful for the style guide that Angular offers. This way, the communication between the team becomes comfortable and less likely to be misinterpreted.

Thanks to the robust ecosystem of the framework that has exceptionally given the power of dependency injection and other resources to the developer communities. 

### Model-View-Viewmodel (MMVC) Architecture to Unify Independent Structures
Angular simplifies the development structure by combining MMVC architecture and two-way data binding. In fact, it takes very little time for this framework to update the changes made in the view layer to the data and vice-versa. Consequently, it is why most developers favor Angular when it comes to building large-scale applications. 

Given that the framework isolates the business logic from the UI components, developers are more confident in designing the neat user interface with smooth business logic applicable. Thanks to the quickest communication established between the model and the view component by the controller. Because of which display of data happens as quickly as possible. 

### Ivy Renderer – Next-Generation Renderer for Enhanced Performance
This new Angular engine is all about its super-optimized bundle size, loading speed, and dynamic loading of components. 

Ivy renderer aims for unparalleled code debugging and user-friendly app experience. It makes the framework accessible and sets an example by reducing the file sizes though making the framework a feature-rich platform. Since it was introduced in NG-CONF 2018, then it wasn't in its complete form, but currently, Angular 8 serves the best of it. The modern Angular engine will be in its full form with the release of Angular 9. 

### Active Community &amp; Easy-to-Access Information

![image info](/img/webdevelopment/6/DTD5Xh0VwAAKfZf.jpg)

Ever since its onset, Angular has been a favorite among developers and engineers for its myriad of advantages. With a pool of resources like study materials, reference documents, FAQs, and others, Angular is easy to integrate for those who want to make the most out of it.

### Prominent communities for Angular to join:
**Dev.to** has got a fantastic community for developers to join 

**Stackoverflow** is an all-time favorite for Devs

Twitter has got many supporting handles for Angular, but you will find everything right from the Angular in this 

**AngularAir** for latest updates and speakers 

You can subscribe to the **ng-newsletter** for updates right in your inbox!

### Improve Productivity with Code Reusability

What can be more precious for developers to build applications regardless of worrying about huge file sizes? 

With Angular, developers invest less time and effort, for it lets them reuse codes and streamlines the development process. It results in bringing about more functionality with shorter codes, making it. Productive for the teams working on back to back similar projects.

### Design Development Workflow

With this advantage, coders can refrain from CSS/HTML markup when they are developing apps and add markups without breaking the application. Instead of rearranging codes during development, all they have to do is move corresponding elements around the code.

### Easy to Use

Angular offers flexibility in coding like no other framework because of its simple design architecture. In terms of flexibility, it allows beginners to get started and make modifications in their application projects seamlessly. Also, Angular handles heavy web applications that are loaded with features and components exceptionally well. What more can a developer ask for?

So, these were the advantages of Angular, and by now, we are confident you would have understood the potential of Angular for your web development needs. 

## When to Use Angular
Angular as a full software stack is massive. It enables developers to build as many applications as they want with the easiest efforts involved. Let's look at some of the popular use cases: 

### Video streaming apps like Youtube PlayStation three app 
Anyone fond of playing video games wouldn't have missed YouTube's version on PlayStation 3. Considering the massive popularity of YouTube, it would require a framework 

Since Google owns Angular, they leveraged it to launch a brand-new version of Youtube on PlayStation 3. The versatile framework has helped Google redesign the PS3 experience by replacing the conventional operating approach to a thoroughly modern one. They replaced mouse movements to basic keystrokes. With Angular accessibility, it is possible to leverage the functionalities of native apps like Youtube PS3 extensively. 

Angular contributed to a greater extent in creating the smooth scrolling and video playback on the YouTube PS3. It is the latest ultra-modern and streamlined experience delivered by Angular and is witnessed by the gamers crowd. 

### eCommerce applications like T-Mobile 
T-Mobile discovered that its customers had to wait for content to load while connecting to the app. So to make it faster and less troublesome, they employed Angular to reduce the time with server-side rendering. 

Furthermore, they employed Angular so they could use HTML parsing for search engine purposes. They also updated the site autonomously and seamlessly implemented the dynamic page composition. 

This leading telecom leader got massive success in their eCommerce app, which received remarkable search engine traffic. With Angular, they revamped the application layout and converted it into a smooth performing app. 

### Real-time Data Application like weather.com 
One of the popular websites for real-time weather updates is weather.com. The credit for its success goes to Angular's versatile capabilities of modules and directories that enable it to load on the desktop and smartphone devices to utilize the widget's features.The framework allows the developers to create flexible widgets based on the custom needs of devices. Fetching the real-time data and dynamically showcasing it made it to the internet with the use of Angular technology. 

### User-generated Content Portals– Upwork and New York Times.
User-generated websites demand a high level of security and management. Angular has made it simple by introducing inner HTML to display the user-generated content and a built-in sanitizer – DomSanitizer – as a security feature. However, it may be a trouble when you use the elements with inline styles because it removes the styles when you do that. 

Angular supports big technicalities that are to be handled and required by user-generated content websites. For example, Upwork as a UGC website and websites that need infinite scrolling like the New York Times are two famous examples having Angular as a back to handle their heavy pages. These websites seem easily fetched and rendered in seconds without sacrificing their single page application features. 

### Websites such as The Guardian for the latest news and content
The Guardian is loaded with rich information and is updated all the time. Thanks to the Angular tech stack for setting an example for using RxJS extensions. It allows developers to create the infinite scrolling of search results. The massive that the Guardian is holding is of another level, but it all runs smoothly with the Angularbackend.

## When not to Use Angular
Angular is superheroic, but still, there are places where using Angular may not benefit you.

Let's see what they are?

### Websites with Static Content:
Small websites with lesser objectives and static content. These types of websites are better built with limited tech stacks and need not use Angular since the technology will only add extra chunks of code, which is not required by the website or a particular project. Example: Landing pages, informative websites, event pages, etc. 

### Limited Resources and Startups:
For small teams and limited resources, Angular is not a good idea to start with. Angular demands skillful available resources for quick problem-solving in large scale projects. Also, the technology should not be used for small projects; they can be built with other available frameworks and need not much facilitations with technicalities. 

Microservices design chooses to be flexible in coding and lets you choose the tools irrespective of the different modules of a project. But Angular is a complete solution with all the tooling required and hence, does not provide flexibility to choose other external tools. You need to carry all the things that are even not required for the project. So if you are planning to use microservice design architecture and still want to use Angular, you should iterate your planning. 

### Game Apps &amp; Heavy Analytical Apps
Angular is not for Gaming apps. Other frameworks, like React, can be used to create highly interactive UI in games. It is because of Angular checks each document object model before it changes its place. 

The framework doesn't work well with heavy and complex mathematical analytics like gaming applications. Although Linkedin, Upwork, Netflix are built with Angular, when it comes to too complex to handle Maths, do not choose Angular. 

## Conclusion
Our frontend developers love this all-time leading framework because of the consistent code structure it maintains,two-way data binding, code reusability to using plain old javascript object models. 

Angular enables developers to write code with simpler structures without involving third parties or external support. It simplifies writing code with better facilitation of testing configurations. 

I've tried to cover all the reasons that our Angular team at we gives us for why they're glued with this framework.</content><author><name>Girish Godage</name></author><category term="learning" /><summary type="html">Key Benefits of Angular &amp;amp; Use Cases</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/img/web_development.png" /></entry></feed>